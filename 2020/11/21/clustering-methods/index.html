<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>几个聚类算法 | J. Xu</title><meta name="author" content="Jinzhong Xu"><meta name="copyright" content="Jinzhong Xu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本节介绍一些典型的聚类算法，如 K-Means，DBSCAN，谱聚类，层次聚类，optics，birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量）之间">
<meta property="og:type" content="article">
<meta property="og:title" content="几个聚类算法">
<meta property="og:url" content="https://xujinzh.github.io/2020/11/21/clustering-methods/index.html">
<meta property="og:site_name" content="J. Xu">
<meta property="og:description" content="本节介绍一些典型的聚类算法，如 K-Means，DBSCAN，谱聚类，层次聚类，optics，birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量）之间">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xujinzh.github.io/img/c5.jpeg">
<meta property="article:published_time" content="2020-11-21T03:01:15.000Z">
<meta property="article:modified_time" content="2024-01-30T15:35:56.092Z">
<meta property="article:author" content="Jinzhong Xu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="clustering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xujinzh.github.io/img/c5.jpeg"><link rel="shortcut icon" href="/img/letter-j.png"><link rel="canonical" href="https://xujinzh.github.io/2020/11/21/clustering-methods/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '几个聚类算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-30 23:35:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/silence.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">414</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">320</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="J. Xu"><img class="site-icon" src="/img/letter-j.png"/><span class="site-name">J. Xu</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">几个聚类算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-21T03:01:15.000Z" title="发表于 2020-11-21 11:01:15">2020-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-30T15:35:56.092Z" title="更新于 2024-01-30 23:35:56">2024-01-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/research/">research</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/research/machine-learning/">machine learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="几个聚类算法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>本节介绍一些典型的聚类算法，如 K-Means，DBSCAN，谱聚类，层次聚类，optics，birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量）之间的距离的，有衡量两个子集合之间的距离的，也有衡量点到子集合之间距离的。</p>
<span id="more"></span>

<p>首先先给出一个图，来直观看一下各个聚类算法的效果图。</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png"></p>
<p>数学上，什么是距离？非空集合 $X$ 上的度量为一个函数（称之为“距离函数”或简称为“距离”）<br>$$<br>d: X \times X \to \mathbb{R}<br>$$<br>这里的 $\mathbb{R}$ 是实数集合，且对于所有 $X$ 内的 $x, y, z$，均满足如下条件：</p>
<ul>
<li><p>非负性，或分离公理<br>$$<br>d(x, y) \geq 0<br>$$</p>
</li>
<li><p>同一性，或同时公理<br>$$<br>d(x,y) &#x3D; 0 \Longleftrightarrow x &#x3D; y<br>$$</p>
</li>
<li><p>对称性<br>$$<br>d(x, y) &#x3D; d(y, x)<br>$$</p>
</li>
<li><p>次加性，或三角不等式<br>$$<br>d(x, z) \leq d(x, y) + d(y, z)<br>$$<br>其次，给出几种计算相似度或距离的方法。</p>
</li>
</ul>
<ol>
<li><p>闵可夫斯基（或叫明氏）距离（Minkowski，欧式空间中的一种距离，$p$-范数距离，$p$不一定要是整数，但不可以小于1，不然三角不等式不成立）<br>$$<br>d(\vec{x}, \vec{y}) &#x3D; (\sum_{i&#x3D;1}^n |x_i - y_i|^p)^{\frac{1}{p}}<br>$$<br>当 $p &#x3D; 1$ 时，称为曼哈顿距离（Manhattan，1-范数距离），$p &#x3D; 2$ 时，称为欧式（欧几里得）距离（Euclidean，2-范数距离），$p &#x3D; +\infty$ 时称为切比雪夫距离（Chebyshev，无穷范数距离）。</p>
</li>
<li><p>杰卡德（Jaccard）相似系数<br>$$<br>J(A, B) &#x3D; \frac{|A \cap B|}{|A \cup B|}<br>$$</p>
</li>
<li><p>余弦相似度（cosine similarity）<br>$$<br>cos(\theta) &#x3D; \frac{\vec{x}^T \vec{y}}{|\vec{x}| \cdot |\vec{y}|}<br>$$</p>
</li>
<li><p>Pearson 相似系数<br>$$<br>\rho_{\vec{x}\vec{y}} &#x3D; \frac{cov(\vec{x}, \vec{y})}{\sigma_{\vec{x}}\sigma_{\vec{y}}}  &#x3D; \frac{E[(\vec{x} - \mu_{\vec{x}})(\vec{y} - \mu_{\vec{y}})]}{\sigma_{\vec{x}}\sigma_{\vec{y}}}<br>$$</p>
</li>
<li><p>相对熵（K-L散度，K-L距离）<br>$$<br>D(p | q) &#x3D; \sum_{\vec{x}} p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}} &#x3D; E_{p(\vec{x})} \log{\frac{p(\vec{x})}{q(\vec{x})}}<br>$$</p>
</li>
<li><p>Hellinger 距离<br>$$<br>D_{\alpha}(p | q) &#x3D; \frac{2}{1-\alpha^2}(1 - \int p(x)^{\frac{1+\alpha}{2}} q(x)^{\frac{1-\alpha}{2}} \mathrm{d}x)<br>$$</p>
</li>
</ol>
<h1 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h1><p>k-means 算法是使用最多，且相对简单的聚类算法。</p>
<p>k-means 算法，也被称为 k-平均或k-均值，是一些聚类算法的基础，如谱聚类。</p>
<p>假设输入样本为 $S &#x3D; x_1, x_2, \cdots, x_m$，则算法步骤为</p>
<ol>
<li><p>选择初始的 $k$ 个类别中心 $\mu_1, \mu_2, \cdots, \mu_k$</p>
</li>
<li><p>对于每个样本 $x_i$，将其标记为距离类别中心最近的类别，即<br>$$<br>label_i &#x3D; \underset{1 \leq j \leq k}{\arg \min} | x_j - \mu_j |<br>$$</p>
</li>
<li><p>将每个类别中心更新为隶属该类别的所有样本的均值<br>$$<br>\mu_j &#x3D; \frac{1}{|c_j|} \underset{i \in c_j}{\sum} x_i<br>$$</p>
</li>
<li><p>重复最后两步，直到类别中心的变化小于某阈值。或者迭代次数达到某阈值，或者最小平方误差MSE（Minimum Squared Error）小于某阈值。</p>
</li>
</ol>
<p>在算法中，如果不取均值，改为取中位数，称为 k-mediods 聚类（k中值聚类）。</p>
<p>初始值的选择对 k-means 算法影响很大，通过 k-means++ 可以有效的选择初始聚类中心。具体的，</p>
<ol>
<li>首先任意选择一个初始样本点 $\mu_1 &#x3D; x_1$ 为第一个聚类中心</li>
<li>其次，计算剩余点 $y_1, y_2, \cdots, y_m$ 到聚类中心 $\mu_1$ 的聚类，假设为 $d_1, d_2, \cdots, d_m$，那么依概率 $\frac{d_1}{d}, \frac{d_2}{d}, \cdots, \frac{d_m}{d}$ 来选择样本点 $y_1, y_2, \cdots, y_m$ 中的一个作为第二个聚类中心 $\mu_2$，其中 $d &#x3D; \sum_{i&#x3D;1}^m d_i$</li>
<li>重复步骤2，直到选到所需的 $k$ 个聚类中心 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>最后，使用 k-means 聚类算法在数据集 $x_1, x_2, \cdots, x_n$ 和  $k$ 个聚类中心 $\mu_1, \mu_2, \cdots, \mu_k$ 上进行迭代聚类</li>
</ol>
<p>关于簇数即聚类中心个数的选择，一种方法是遍历簇数取值 $1, 2, \cdots, n$，画图查看不同簇数取值下整个数据集 MSE 变化情况，即从数据集方差（当类簇数等于1）下降到0（当类簇数等于 $n$，即样本个数，每个样本聚成一个类），选择 MSE 变化的“肘关节点”；另一种方法是，当选择某个 $k$ 作为聚类中心树，进行 k-means 聚类，计算聚类后每个类簇的 MSE，对于取值较大 MSE 的类簇增加聚类中心个数，对于取值较小的，合并聚类中心。</p>
<p><font color='dd00dd'> k-means 深层解释 </font></p>
<p>假设样本集 ${ x_i }$  有 $k$ 个簇，每个簇样本数目为 $N_1, N_2, \cdots, N_k$，且每个簇都服从相同方差 $\sigma$ 的高斯分布，每个类簇的均值就是聚类中心 $\mu_i$，那么，k-means 聚类的最大似然函数是<br>$$<br>\Pi_{j&#x3D;1}^k \Pi_{i&#x3D;1}^{N_j} \frac{1}{\sqrt{2\pi}\sigma} \exp ^{-\frac{(x_i^{j} - \mu_j)}{2\sigma^2}}<br>$$<br>取对数后，只保留参数 $\mu_i$ 部分<br>$$<br>J(\mu_1, \mu_2, \cdots, \mu_k) &#x3D; \frac{1}{2} \sum_{j&#x3D;1}^k \sum_{i&#x3D;1}^{N_j} (x_i^j - \mu_j)^2<br>$$<br>对函数 $J$ 关于参数变量 $\mu_1, \mu_2, \cdots, \mu_k$ 求偏导，得到<br>$$<br>\frac{\partial J}{\partial \mu_j} &#x3D; - \sum_{i&#x3D;1}^{N_j}(x_i^j - \mu_j) &#x3D; 0 \Rightarrow \mu_j &#x3D; \frac{1}{N_j} \sum_{i&#x3D;1}^{N_j} x_i^j<br>$$<br>由此可以知道，k-means 运行的隐含假设是，每个类簇都服从高斯分布，且方差相同。即数据集服从方差相同的混合高斯分布。同时，该算法的目标函数是 MSE，且采用梯度下降算法进行迭代优化。因此，机器学习中非凸函数梯度下降算法的问题，该算法都会有，比如，初始点（初始聚类中心）的选择，局部最优问题，收敛震荡问题，鞍点问题等。同时，也提醒我们，可以使用 SGD 来求解 k-means（即 Mini-batch k-means 算法，不是把每个类的算有点个数 $N_j$ 都使用来计算均值，而是随机使用部分点 $N_j^{\prime} &lt; N_j$ 来计算均值），同样的，也可以采用 k-means++ 筛选初始点的方法来选择机器学习方法初始点。</p>
<p><font color='dd00dd'> k-means 聚类算法的优缺点 </font></p>
<p>优点：</p>
<ol>
<li>是解决聚类问题的一种经典算法，简单，快速</li>
<li>对处理大数据集，该算法保持可伸缩性和高效率</li>
<li>当簇近似分为高斯分布时，效果较好</li>
<li>可作为其他聚类方法的基础，如谱聚类</li>
</ol>
<p>缺点：</p>
<ol>
<li>在簇的平均值可被定义的情况下才能使用，可能不适应于某些应用</li>
<li>必须事先给出 $k$ 值（要生成的簇个数），而且对初值敏感，对于不同的初始值，可能会导致不同结果</li>
<li>不适合于发现非凸形状的簇或者大小差别很大的簇</li>
<li>对噪声和孤立点数据敏感</li>
</ol>
<p><strong>KMeans 算法的重要参数包括：n_clusters​.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">10</span>, <span class="number">2</span>], [<span class="number">10</span>, <span class="number">4</span>], [<span class="number">10</span>, <span class="number">0</span>]])</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line">kmeans.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], dtype=int32)</span><br><span class="line"></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"><span class="comment"># 聚类中心</span></span><br><span class="line">array([[<span class="number">10.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], dtype=int32)</span><br></pre></td></tr></table></figure>

<p><strong>MiniBatchKMeans 算法的重要参数包括：n_clusters, batch_size​.</strong></p>
<p>MiniBatchKMeans 一般会比 KMeans 结果差，但是运行更快。</p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"></span><br><span class="line">X = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">1</span>, -<span class="number">1</span>],</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">kmeans = MiniBatchKMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>, batch_size=<span class="number">6</span>, max_iter=<span class="number">10</span>).fit(X)</span><br><span class="line">kmeans.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)</span><br><span class="line"></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"><span class="comment"># 聚类中心</span></span><br><span class="line">array([[<span class="number">3.95918367</span>, <span class="number">2.40816327</span>],</span><br><span class="line">       [<span class="number">1.12195122</span>, <span class="number">1.3902439</span> ]])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MiniBatchKMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>, batch_size=<span class="number">6</span>, max_iter=<span class="number">10</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)</span><br></pre></td></tr></table></figure>



<h1 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）算法是一种基于密度的聚类算法。密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆形”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。</p>
<p>DBSCAN 是一个比较有代表性的基于密度的聚类算法。与下面将要介绍的层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。</p>
<p>介绍 DBSCAN 需要了解如下几个概念：</p>
<ol>
<li>对象的 $\varepsilon$-邻域：给定对象在半径 $\varepsilon$ 内的区域。</li>
<li>核心对象：对于给定的数目 $m$，如果一个对象的 $\varepsilon$-邻域至少包含 $m$ 个对象，则称该对象为核心对象。</li>
<li>直接密度可达：给定一个对象集合 $D$，如果 $p$ 是在 $q$ 的 $\varepsilon$-邻域内，而 $q$ 是一个核心对象，我们说对象 $p$ 从对象 $q$ 出发是直接密度可达的。 </li>
<li>密度可达：如果存在一个对象链 $p_1, p_2, \cdots, p_n, p_1 &#x3D; q, p_n &#x3D; p$，对 $p_i \in D, (i \leq i \leq n)$,  $p_{i+1}$ 是从 $p_i$ 关于 $\varepsilon$ 和 $m$ 直接密度可达的，则对象 $p$ 是从对象 $q$ 关于 $\varepsilon$ 和 $m$ 密度可达的。</li>
<li>密度相连：如果对象集合 $D$ 中存在一个对象 $o$，使得对象 $p$ 和 $q$ 是从 $o$ 关于 $\varepsilon$ 和 $m$ 密度可达的，那么对象 $p$ 和 $q$ 是关于 $\varepsilon$ 和 $m$ 密度相连的。</li>
<li>簇：一个基于密度的簇是最大的密度相连对象的集合。</li>
<li>噪声：不包含在任何簇中的对象称为噪声。</li>
</ol>
<p>DBSCAN 算法流程：</p>
<ol>
<li>如果一个点 $p$ 的 $\varepsilon$-邻域包含多于 $m$ 个对象，则创建一个 $p$ 作为核心对象的新簇；</li>
<li>寻找并合并核心对象直接密度可达的对象；</li>
<li>没有新点可以更新簇时，算法结束。</li>
</ol>
<p>从算法流程可以看出，每个簇至少包含一个核心对象。非核心对象可以是簇的一部分，构成了簇的边缘。包含过少对象的簇被认为是噪声。DBSCAN 不需要事先给定聚类个数。对于大数据集因为要计算密度，不太适用。</p>
<p>DBSCAN 优势：</p>
<ol>
<li>不需要指定簇的个数</li>
<li>可以发现任意形状的簇</li>
<li>擅长找到离群点（检测任务）</li>
<li>需要的参数较少</li>
</ol>
<p>DBSCAN 劣势：</p>
<ol>
<li>高纬数据处理比较困难（可以先降维）</li>
<li>参数难以选择（参数对结果的影响非常大）</li>
<li>scikit-learn 中效率很慢（使用数据消减策略）</li>
</ol>
<p><strong>DBSCAN 算法的重要参数包括：$\varepsilon$&#x3D;eps, m&#x3D;min_samples​.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">8</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">25</span>, <span class="number">80</span>]])</span><br><span class="line">clustering = DBSCAN(eps=<span class="number">3</span>, min_samples=<span class="number">2</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>, -<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DBSCAN(eps=<span class="number">3</span>, min_samples=<span class="number">2</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>, -<span class="number">1</span>])</span><br></pre></td></tr></table></figure>



<h1 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h1><p>谱聚类通过计算矩阵的特征向量来实现。可以把谱聚类描述为 PCA + K-means.</p>
<p>谱聚类是一种基于图论的聚类方法，通过对样本数据的拉普拉斯矩阵的特征向量进行聚类，从而达到对样本数据聚类的目的。可以解决区域重叠问题。</p>
<p>谱分析的整体过程：</p>
<ol>
<li>给定一组数据 $x_1, x_2, \cdots, x_n$，记任意两个点之间的相似度（“距离” 的负函数）为 $s_{ij} &#x3D; &lt;x_i, x_j&gt;$，形成相似度图：$G&#x3D;(V, E)$. 如果 $x_i$ 和 $x_j$ 之间的相似度 $s_{ij}$ 大于一定的阈值，那么，两个点是连接的，权值记作 $s_{ij}$.</li>
<li>接下来，可以用相似度图来解决样本距离问题：找到图的一个划分，形成若干个组，使得不同组之间有较低的权值，组内有较高的权值。</li>
</ol>
<p><strong>谱聚类的流程：</strong></p>
<ol>
<li><p>计算任意两个样本之间的相似度，这里常采用高斯相似度 $w_{ij} &#x3D; exp(-\frac{|x_i - x_j|^2}{2\sigma^2})$, 这里 $\sigma$ 是超参数；</p>
</li>
<li><p>构造相似度矩阵 $W &#x3D; (w_{ij})$， 需要注意的是，主对角线元素本来是 $w_{ii} &#x3D; 1$，这里替换成 0；</p>
</li>
<li><p>构造度矩阵 $D &#x3D; diag(d_i)$，这里 $d_i &#x3D; \sum_{i&#x3D;1}^n w_{ij}$;</p>
</li>
<li><p>构造拉普拉斯矩阵 $L &#x3D; D - W$，如二维的例子对应的拉普拉斯矩阵如下</p>
<p>$$D &#x3D; \begin{bmatrix}<br>w_{12} + w_{13} &amp; 0 &amp; 0 \\<br>0 &amp; w_{21} + w_{23} &amp; 0 \\<br>0 &amp; 0 &amp; w_{31} + w_{32}<br>\end{bmatrix}$$  </p>
<p> $$W &#x3D; \begin{bmatrix}<br>0 &amp; w_{12} &amp; w_{13} \\<br>w_{21} &amp; 0 &amp; w_{23} \\<br>w_{31} &amp; w_{32} &amp; 0<br>\end{bmatrix} $$ </p>
<p>$$L &#x3D; \begin{bmatrix}<br> w_{12} + w_{13} &amp; -w_{12} &amp; -w_{13} \\<br> -w_{21} &amp; w_{21} + w_{23} &amp; -w_{23} \\<br> -w_{31} &amp; -w_{32} &amp; w_{31} + w_{32}<br> \end{bmatrix}$$</p>
</li>
<li><p>求拉普拉斯矩阵的谱，即所有的特征向量，并从小到大进行排列，假设为 $\lambda_1, \lambda_2, \cdots, \lambda_k, \cdots, \lambda_n$, 其对应的特征向量分别为 $\mu_1, \mu_2, \cdots, \mu_k &#x3D; (\mu_{ik})^T, \cdots, \mu_n$，其中，特征向量<br>$$<br>\begin{bmatrix}<br>\mu_{11} &amp; \mu_{12} &amp; \cdots &amp; \mu_{1k} &amp; \cdots &amp; \mu_{1n} \\<br>\mu_{21} &amp; \mu_{22} &amp; \cdots &amp; \mu_{2k} &amp; \cdots &amp; \mu_{2n} \\<br>\vdots   &amp; \vdots   &amp; \ddots &amp; \vdots   &amp; \ddots &amp; \vdots   \\<br>\mu_{n1} &amp; \mu_{n2} &amp; \cdots &amp; \mu_{nk} &amp; \cdots &amp; \mu_{nn}<br>\end{bmatrix}<br>$$</p>
<p>的每一行就是样本映射的特征。选取前 $k$ 列个特征向量（前 $k$ 小特征值对应的特征向量）作为主特征向量（类似 PCA），然后，对矩阵<br>$$<br>\begin{bmatrix}<br>\mu_{11} &amp; \mu_{12} &amp; \cdots &amp; \mu_{1k} \\<br>\mu_{21} &amp; \mu_{22} &amp; \cdots &amp; \mu_{2k} \\<br>\vdots   &amp; \vdots   &amp; \ddots &amp; \vdots   \\<br>\mu_{n1} &amp; \mu_{n2} &amp; \cdots &amp; \mu_{nk}<br>\end{bmatrix}<br>$$<br>按行聚类，采用聚类方法 k-means，这样就得到了原始样本的谱聚类。</p>
</li>
</ol>
<p><strong>证明，拉普拉斯矩阵 $L &#x3D; D - W$ 是半正定矩阵。</strong></p>
<p>对任意非零向量 $f$<br>$$<br>f^T L f &#x3D; f^T D f - f^T W f &#x3D; \sum_{i&#x3D;1}^n d_i f_i^2 - \sum_{i,j&#x3D;1}^n f_i f_j w_{ij} \\<br>&#x3D; \frac{1}{2} (\sum_{i&#x3D;1}^n d_i f_i^2 - 2 \sum_{i,j&#x3D;1}^n f_i f_j w_{ij} + \sum_{j&#x3D;1}^n d_j f_j^2) \\<br>&#x3D; \frac{1}{2} \sum_{i,j&#x3D;1}^n w_{ij}(f_i - f_j)^2 \geq 0.<br>$$<br>$L$ 是对称半正定矩阵，最小特征值是0， 相应的特征向量是全1向量。其他特征值大于零。</p>
<p>它适用于少数群集，但不建议用于许多群集。</p>
<p><font color='dd0000'>谱聚类算法：未正则拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i&#x3D;1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li>计算拉普拉斯矩阵 $L &#x3D; D - W$</li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i &#x3D; 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i&#x3D;1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i &#x3D; {j|y_j \in C_i}$</li>
</ul>
<p><font color='dd0000'>谱聚类算法：随机游走拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i&#x3D;1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li><font color='dd00dd'>计算拉普拉斯矩阵 $L_{rw} &#x3D; D^{-1}(D - W)$</font></li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i &#x3D; 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i&#x3D;1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i &#x3D; {j|y_j \in C_i}$</li>
</ul>
<p><font color='dd0000'>谱聚类算法：对称拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i&#x3D;1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li><font color='dd00dd'>计算拉普拉斯矩阵 $L_{sym} &#x3D; D^{-1&#x2F;2}(D - W)D^{-1&#x2F;2}$</font></li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i &#x3D; 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li><font color='dd00dd'>对于 $i&#x3D;1,2,\cdots, n$，将$y_i \in \mathbb{R}^k $ 依次单位化，使得 $|y_i| &#x3D; 1$ </font></li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i&#x3D;1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i &#x3D; {j|y_j \in C_i}$</li>
</ul>
<p><strong>有时候，也会使用拉普拉斯矩阵 $L &#x3D; W - D$，或者 $L &#x3D; D^{-1}W$ (因为 $L &#x3D; D^{-1}(D-W) &#x3D; I - D^{-1}W$，不考虑单位矩阵效果一样)，此时，需要对特征值进行从大到小排列，选取相应的特征向量。</strong></p>
<p><em>当未给定先验知识的情况下，优先选择随机游走拉普拉斯矩阵。对于 $n$ 个样本，通过相似度构建一个相似度连通图，共有 $n(n+1)&#x2F;2$ 条边，边上的权重就是两个样本之间的相似度。那么谱聚类的目标函数就是使用剪刀剪断一个边使得损失值最小（即特征值）。</em></p>
<p>随机游走和拉普拉斯矩阵的关系（当拉普拉斯矩阵 $L &#x3D; D^{-1}W$ 时）</p>
<p>图论中的随机游走是一个随机过程，它从一个顶点跳转到另外一个顶点。谱聚类即找到图的一个划分，使得随机游走在相同的簇中停留而几乎不会游走到其他簇。转移矩阵是 $P&#x3D;D^{-1}W$，表示从顶点 $v_i$ 跳转到顶点 $v_j$ 的概率正比于边的权值，即 $p_{ij} &#x3D; w_{ij}&#x2F;d_i$.</p>
<p>进一步的，可以考虑：</p>
<ul>
<li>谱聚类中的 $k$ 如何确定？$k^{\star} &#x3D; \arg \max_k |\lambda_{k+1} - \lambda_k|$</li>
<li>最后一步的 $k-means$ 的作用是什么？目标函数是关于子图划分指示向量的函数，该向量的值根据子图划分确定，是离散的。该问题是 NP 的，转化成求连续实数域上的解，最后用 $k-means$ 算法离散化</li>
<li>未正则、对称、随机游走拉普拉斯矩阵，首先哪个？随机游走拉普拉斯矩阵</li>
<li>谱聚类可以用切割图，随机游走，扰动论等解释</li>
</ul>
<p><strong>谱聚类算法的重要参数包括：n_clusters, assign_labels, eigen_solver​.</strong></p>
<p>这里 n_cluster 表示聚类个数，也表示投影子空间的维数。eigen_solver 表示矩阵分解求特征值的策略，可选取值有 $None, arpack, lobpcg, or\ amg$, 其中， $amg$ 需要安装 $pyamg$ 才能运行，该分解策略能够有效提高大数据集、稀疏问题的计算速度，但可能会导致结果的不稳定。assign_labels​ 用于在嵌入空间中分配标签的策略。拉普拉斯嵌入后，有两种分配标签的方法。可以应用 <strong>kmeans</strong>，它是一种流行的选择。但是它也可能对初始化敏感。<strong>discretize</strong> 是另一种对随机初始化不太敏感的方法。</p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> SpectralClustering</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">7</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line">clustering = SpectralClustering(</span><br><span class="line">    n_clusters=<span class="number">2</span>, assign_labels=<span class="string">&quot;discretize&quot;</span>, random_state=<span class="number">0</span></span><br><span class="line">).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SpectralClustering(</span><br><span class="line">    n_clusters=<span class="number">2</span>, assign_labels=<span class="string">&quot;discretize&quot;</span>, random_state=<span class="number">0</span></span><br><span class="line">).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>



<h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>层次聚类（Hierarchical clustering）方法对给定的数据集进行层次分解，直到满足某种条件为止。具体地，可分为凝聚式层次聚类和分裂式层次聚类。</p>
<h2 id="凝聚式层次聚类"><a href="#凝聚式层次聚类" class="headerlink" title="凝聚式层次聚类"></a>凝聚式层次聚类</h2><p>凝聚式层次聚类又叫作 AGNES 算法。该算法属于层次聚类常采用的算法。相对于下面分裂式层次聚类，该算法复杂度更小。</p>
<p>scikit-learn 中实现的层次聚类算法 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering">AgglomerativeClustering</a>, 使用自底向上执行层次聚类。每个样本都从自己聚类开始，并且聚类被连续合并在一起。合并或链接的标准成为算法的关键点。实现的合并策略的链接标准有 Ward （最小化所有类平方差和，一种方差最小化方法，某种意义上与k -means相似）、Maximum or complete linkage（最小化两对类簇的最大距离）、Average linkage（最小化两对类簇所有观测样本的距离平均值）、Single linkage（最小化两对类簇之间的最小观测样本距离）。不同策略得到的层次聚类效果图如下：</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png"></p>
<ol>
<li>最小距离<ul>
<li>两个集合中最近的两个样本的聚类，single</li>
<li>容易形成链状结构</li>
</ul>
</li>
<li>最大距离<ul>
<li>两个集合中最远的两个样本的距离，maximum or complete</li>
<li>若存在异常值则不稳定</li>
</ul>
</li>
<li>平均距离<ul>
<li>两个集合中样本间两两距离的平均值，average</li>
<li>两个集合中样本间两两距离的平方和，ward</li>
</ul>
</li>
</ol>
<p>注意，Agglomerative 聚类具有一种 “富人更富有” 的行为，这将导致聚类大小不均匀。在这方面，single linkage 策略是最糟糕的，ward 能够给出最公正的大小。但是，ward 无法更改相似性（或者距离），因此对于非欧度量，average linkage 是一个更好的选择。single linkage 虽然对噪声敏感，但是计算效率高，能够推广到大数据集上。同时，在非球形数据上表现良好。</p>
<p>层次聚类能够给出数据结构的可视化信息，如类似下面的树状图：</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png"></p>
<p>当与连接矩阵一起使用时，AgglomerativeClustering 也可以扩展到大量样本，但是当在样本之间不添加连接限制时，计算开销很大，因为它在每一步都考虑了所有可能的合并。</p>
<p>AgglomerativeClustering 有趣的方面是，可以通过一个连通性矩阵将连通性约束添加到该算法中（只能将相邻的聚类合并在一起），该矩阵为每个样本定义遵循给定数据结构的相邻样本。例如，在下面的瑞士卷示例中，连接性约束禁止合并瑞士卷上不相邻的点，因此避免形成在卷的重叠折叠部分延伸的簇。</p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png"></p>
<p><img src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png"></p>
<p>这些约束对于强加特定的局部结构很有用，同时它们也使算法更快，尤其是在样本数量很多时。</p>
<p><strong>层次聚类（这里指 Agglomerative Clustering）的重要参数：n_clusters, linkage.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line">clustering = AgglomerativeClustering(n_clusters=<span class="number">2</span>, linkage=<span class="string">&#x27;ward&#x27;</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">AgglomerativeClustering(n_clusters=<span class="number">2</span>, linkage=<span class="string">&#x27;ward&#x27;</span>).fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="分裂式层次聚类"><a href="#分裂式层次聚类" class="headerlink" title="分裂式层次聚类"></a>分裂式层次聚类</h2><p>分裂式层次聚类是 DIANA 算法。采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。</p>
<h1 id="Birch"><a href="#Birch" class="headerlink" title="Birch"></a>Birch</h1><p>Birch 算法叫做平衡迭代削剪聚类算法，它是对层次聚类算法的一种优化，能够处理大数据集。</p>
<p>Birch 使用三元组的聚类特征表示类簇。通过构建满足分支因子和簇直径限制的聚类特征树（Clustering Feature Tree，CFT）来求聚类，数据本质上是有损压缩到一组“群集特征”节点（CF节点）的。聚类特征树其实是一个具有两个参数<font color='dd0000'>分支因子</font>和<font color='dd0000'>类直径</font>的高度平衡树。分支因子规定了树的每个节点的子女的最多个数，而类直径体现了对这一类点的距离范围。非叶子节点为它的子女的最大特征值。聚类特征树的构建是一个动态过程，可以随时根据数据对模型进行更新操作。</p>
<p>假设一个类簇包含的样本点集合为 ${x_1, x_2, \cdots, x_n}$ ，三元组存储的信息分别是 $(n, s_1, s_2)$，其中 $n$ 表示样本点个数，$s_1 &#x3D; x_1 + x_2 + \cdots + x_n$，$s_2 &#x3D; x_1 \cdot x_1 + x_2 \cdot x_2 + \cdots + x_n \cdot x_n$. 根据这个三元组信息，类直径和分支因子可以做如下事情。</p>
<p>类直径：判断样本是否属于当前簇</p>
<p>分支因子：当一个节点的子节点数目超过分支因子的时候，将该节点划分为两个子节点。当一个叶子节点的样本数目超过分支因子的时候，将叶子节点分成两个叶子节点。</p>
<p>优点：</p>
<ol>
<li>适合大规模数据集，线性效率</li>
<li>节约内存，所有样本都在磁盘上，CFT 仅仅存储 CF 节点和对应的指针</li>
<li>可以识别噪声。还可以对数据集进行初步预处理，作为其他聚类算法的输入，如层次聚类</li>
</ol>
<p>缺点：</p>
<ol>
<li>只适合分布呈凸形或者球形的数据集，需要给定聚类个数和簇之间的相关参数</li>
<li>无法很好地缩放到高维数据。根据经验，如果特征维度大于20，通常最好使用 MiniBatchKMeans</li>
<li>由于 CFT 对每个节点的 CF 个数有限制，导致聚类结果可能和真实类别分布不同</li>
</ol>
<p><strong>Birch 算法的重要参数：threshold, branching_factor, n_clusters.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> Birch</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0.3</span>, <span class="number">1</span>], [-<span class="number">0.3</span>, <span class="number">1</span>], [<span class="number">0</span>, -<span class="number">1</span>], [<span class="number">0.3</span>, -<span class="number">1</span>], [-<span class="number">0.3</span>, -<span class="number">1</span>]]</span><br><span class="line">brc = Birch(n_clusters=<span class="literal">None</span>)</span><br><span class="line">brc.fit(X)</span><br><span class="line">brc.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">brc.fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>参考链接</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6179132.html">BIRCH聚类算法原理</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54837341">数据挖掘入门笔记——BIRCH聚类（一拍即合）</a></li>
</ol>
<h1 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h1><p>OPTICS (Ordering Points To Identify the Clustering Structure) 聚类算法是前面 DBSCAN 算法的一种扩展，能够解决 DBSCAN 对超参数敏感（不易选择超参数）的问题。与 DBSCAN 相比，OPTICS 算法的改进主要在于对输入参数不敏感。</p>
<p>OPTICS 算法不显示地生成数据聚类，它只是对数据对象集合中的对象进行排序，得到一个有序的对象列表，其中包含了足够的信息用来提取聚类。</p>
<p>因为 OPTICS 算法是 DBSCAN 算法的改进，因此很多概念可以沿用，如（直接）密度可达，密度相连，核心点，半径参数 $\varepsilon$，最小点数 $M$ 等。假设数据集 $X &#x3D; \lbrace x_1, x_2, \cdots, x_n \rbrace$，</p>
<p><font color='dd00dd'>核心距离（core distance）</font></p>
<p>点 $x \in X$ 称为核心点的最小邻域半径定义为 $x$ 的核心距离，<br>$$<br>\text{cd} (x) &#x3D;<br>\begin{cases}<br>\text{undefined}, &amp; |N_{\varepsilon}(x)|&lt; M \\<br>d(x, y(N_{\varepsilon}^M, x)) &amp; |N_{\varepsilon}(x)| \geq M<br>\end{cases}<br>$$<br>其中 $y(N_{\varepsilon}^i, x)$ 表示集合 $N_{\varepsilon}(x)$ 中与点 $x$ 第 $i$ 近邻的点。</p>
<p>从定义可以知道，如果 $x$ 是核心点，那么核心距离 $0 \leq \text{cd}(x) \leq \varepsilon$.</p>
<p><font color='dd00dd'>可达距离（reachability distance）</font></p>
<p>对于点 $x, y \in X$，$y$ 关于 $x$ 的可达距离定义为<br>$$<br>\text{rd}(y, x) &#x3D;<br>\begin{cases}<br>\text{undefined}, &amp; |N_{\varepsilon}(x)| &lt; M \\<br>\max { \text{cd}(x), d(x, y) } &amp; |N_{\varepsilon}(x)| \geq M<br>\end{cases}<br>$$<br>即 $\text{rd}(y, x)$ 表示使得 $x$ 为核心点且 $y$ 从 $x$ 直接密度可达同时成立的最小邻域半径。</p>
<p>通俗理解就是，对于核心点 $x$ ，与点 $x$ 距离小于等于核心距离的可达距离调整为核心距离，大于核心距离的可达距离就是两点之间的距离。</p>
<p>可达距离 $\text{rd}(y, x)$ 的值与点 $y$ 所在空间的密度有关，密度越大，它从相邻节点直接密度可达的距离就越小。如果聚类时想要朝着数据尽量稠密的空间进行扩展，那么可达距离最小的点是最佳的选择。为此，OPTICS 算法中用一个可达距离<strong>升序排列</strong>的有序种子队列存储带扩展的点，以迅速定位稠密空间的数据对象。</p>
<p>在给出 OPTICS 算法之前，先引入以下定义</p>
<ol>
<li>$p_i, i &#x3D; 1, 2, \cdots, N$：OPTICS 算法的输出序数组，$p_i \in \lbrace 1, 2, \cdots, N \rbrace$ 表示排在第 $i$ 个位置的节点的编号</li>
<li>$r_i, i &#x3D; 1, 2, \cdots, N$：第 $i$ 号节点的可达距离</li>
<li>$c_i, i &#x3D; 1, 2, \cdots, N$：第 $i$ 号节点的核心距离</li>
<li>$v_i, i &#x3D; 1, 2, \cdots, N$：表示节点便函是否被访问过的辅助数组，$0$ 表示未访问过，$1$ 表示访问过。这里是否被访问过是指是否被加入到输出序数组 $p$ 中</li>
</ol>
<p>OPTICS 算法：</p>
<ul>
<li><p>初始化</p>
<ol>
<li>给定参数 $\varepsilon, M$</li>
<li>生成 $N_{\varepsilon}(i), i &#x3D; 1, 2, \cdots, N$</li>
<li>生成 $c_i, i &#x3D; 1, 2, \cdots, N$</li>
<li>令 $v_i &#x3D; 0, i &#x3D; 1, 2, \cdots, N$</li>
<li>令 $r_i &#x3D; \text{undefined}, i &#x3D; 1, 2, \cdots, N$</li>
<li>令 $k &#x3D; 1, I &#x3D; \lbrace 1, 2, \cdots, N \rbrace$</li>
<li>将队列 seedlist 初始化为空</li>
</ol>
</li>
<li><p>主流程</p>
<p>while ($I \neq 0$)</p>
<p>{</p>
<p>​	从 $I$ 中任取一个元素 $i$，令 $I:&#x3D; I \ \lbrace i \rbrace$</p>
<p>​	if ($v_i &#x3D; 0$)  # 表示 $i$ 号节点还没有被处理过</p>
<p>​	{</p>
<p>​		（1）令 $v_i &#x3D; 1$</p>
<p>​		（2）令 $p_k &#x3D; i, k &#x3D; k + 1$</p>
<p>​		（3）若 $|N_{\varepsilon}(i)| \geq M$ (即 $i$ 为核心点)，则</p>
<p>​			（3.1）调用 insertlist($N_{\varepsilon}(i), {v_i}<em>{i&#x3D;1}^N, {r_i}</em>{i&#x3D;1}^N, c_i$, seedlist)，将 $N_{\varepsilon}(i)$ 中未被访问过的节点按照可达距离插入到队列 seedlist 中。（初始化 seedlist）</p>
<p>​			（3.2）while (seedlist not empty)</p>
<p>​						{</p>
<p>​							（a）从 seedlist 中取出<strong>第一个元素</strong> $j$ （其可达距离值最小）</p>
<p>​							（b）令 $v_j &#x3D; 1$</p>
<p>​							（c）令 $p_k &#x3D; j, k &#x3D; k + 1$</p>
<p>​							（d）若 $|N_{\varepsilon}(j)| \geq M$ （即 $j$ 为核心点），则调用 insertlist($N_{\varepsilon}(j), \lbracev_l\rbrace_{l&#x3D;1}^N, \lbrace_l\rbrace_{l&#x3D;1}^N, c_j$, seedlist)，将 $N_{\varepsilon}(j)$ 中未被访问过的节点按照可达距离插入到队列 seedlist 中</p>
<p>​						}</p>
<p>​	}</p>
<p>}</p>
</li>
<li><p>insertlist($N_{\varepsilon}(j), \lbrace v_l\rbrace_{l&#x3D;1}^N, \lbrace r_l\rbrace_{l&#x3D;1}^N, c_j$, seedlist) 模块的算法</p>
<p>for all $J \in N_{\varepsilon}(K)$ do</p>
<p>​	if ($v_J &#x3D; 0$)</p>
<p>​		$\text{rd} &#x3D; \max \lbrace \text{cd}_K, d(x_K, x_J)\rbrace$</p>
<p>​		if ($r_J &#x3D; \text{undefined}$)</p>
<ol>
<li><p>令 $r_j &#x3D; \text{rd}$</p>
</li>
<li><p>将节点 $J$ 按照可达距离值插入到队列 seedlist 的适当位置</p>
<p>else</p>
<p>if ($\text{rd} &lt; r_J$)</p>
<ol>
<li>令 $r_J &#x3D; \text{rd}$</li>
<li>将节点 $J$ 按照可达距离值插入到队列 seedlist 的适当位置</li>
</ol>
</li>
</ol>
</li>
<li><p>得到数据 $\lbrace p_i \rbrace_{i&#x3D;1}^N, \lbrace c_i \rbrace_{i&#x3D;1}^N, \lbrace r_i \rbrace_{i&#x3D;1}^N$ 后，就可以利用它们进行聚类。此时还需要一个邻域半径参数 $\lambda \leq \varepsilon$</p>
</li>
<li><p>与 DBSCAN 类似，引入 cluster 标记数组<br>$$<br>m_i &#x3D;<br>\begin{cases}<br>j(&gt;0), &amp;  x_i 属于第 j 个 \text{cluster} \\<br>-1, &amp; x_i 为噪声<br>\end{cases}<br>$$<br>所以，要得到一个聚类，只需要生成标记数组 $\lbrace m_i \rbrace_{i&#x3D;1}^N$ 即可</p>
</li>
<li><p>生成标记数组（ OPTICS Cluster extracting）</p>
</li>
</ul>
<p>  clusterID &#x3D; -1</p>
<p>  $k &#x3D; 1$</p>
<p>  for $i &#x3D; 1, 2, \cdots, N$, do</p>
<p>  ​	$j &#x3D; p_i$</p>
<p>  ​	if ($r_j &gt; \lambda$ or $r_j &#x3D; \text{undefined}$)</p>
<p>  ​		if ($c_j \neq \text{undefinded}$ and $c_j \leq \lambda$)</p>
<p>  ​			clusterID &#x3D; $k$</p>
<p>  ​			$k &#x3D; k + 1$</p>
<p>  ​			$m_j &#x3D; $ clusterID</p>
<p>  ​		else</p>
<p>  ​			$m_j &#x3D; -1$</p>
<p>  else</p>
<p>  ​	$m_j &#x3D; $ clusterID</p>
<p><strong>OPTICS 算法的重要参数：min_samples.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> OPTICS</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">7</span>, <span class="number">3</span>]])</span><br><span class="line">clustering = OPTICS(min_samples=<span class="number">2</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">OPTICS(min_samples=<span class="number">2</span>).fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>参考链接</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/itplus/article/details/10089323">聚类算法初探（六）OPTICS</a></li>
<li><a target="_blank" rel="noopener" href="https://www.biaodianfu.com/optics.html">密度聚类算法之OPTICS</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41930932">OPTICS聚类算法</a></li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/clustering.html">Scikit-learn Clustering</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://xujinzh.github.io">Jinzhong Xu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://xujinzh.github.io/2020/11/21/clustering-methods/">https://xujinzh.github.io/2020/11/21/clustering-methods/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xujinzh.github.io" target="_blank">J. Xu</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="/tags/clustering/">clustering</a></div><div class="post_share"><div class="social-share" data-image="/img/c5.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/24/markdown-matrix/" title="在 markdown 中书写矩阵"><img class="cover" src="/img/c13.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">在 markdown 中书写矩阵</div></div></a></div><div class="next-post pull-right"><a href="/2020/11/19/cloud-by-filebrowser-and-nginx/" title="使用 filebrowser 和 Nginx 在 VPS 搭建云盘"><img class="cover" src="/img/c9.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">使用 filebrowser 和 Nginx 在 VPS 搭建云盘</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/08/02/cnn-flops/" title="CNN 模型计算力 FLOPs"><img class="cover" src="/img/c6.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-02</div><div class="title">CNN 模型计算力 FLOPs</div></div></a></div><div><a href="/2020/11/03/convolution-in-deep-learning/" title="深度学习中不同的卷积操作"><img class="cover" src="/img/c22.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-03</div><div class="title">深度学习中不同的卷积操作</div></div></a></div><div><a href="/2020/03/26/ml-nn-loss-accuracy/" title="机器学习神经网络模型loss和accuracy的理解"><img class="cover" src="/img/c25.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-26</div><div class="title">机器学习神经网络模型loss和accuracy的理解</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/silence.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jinzhong Xu</div><div class="author-info__description">众妙之门</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">414</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">320</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xujinzh"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xujinzh" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xujinzhong027@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.mathscv.com" target="_blank" title="MathsCVBlog"><i class="fab fa-j"></i></a><a class="social-icon" href="https://xujinzh.github.io" target="_blank" title="GitHubBlog"><i class="fab fa-github-alt"></i></a><a class="social-icon" href="https://www.mathscv.com/power" target="_blank" title="MathsCVPower"><i class="fab fa-m"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">日本核污染水强排入海！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#K-Means"><span class="toc-number">1.</span> <span class="toc-text">K-Means</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DBSCAN"><span class="toc-number">2.</span> <span class="toc-text">DBSCAN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B0%B1%E8%81%9A%E7%B1%BB"><span class="toc-number">3.</span> <span class="toc-text">谱聚类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">4.</span> <span class="toc-text">层次聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%9D%E8%81%9A%E5%BC%8F%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">4.1.</span> <span class="toc-text">凝聚式层次聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E8%A3%82%E5%BC%8F%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="toc-number">4.2.</span> <span class="toc-text">分裂式层次聚类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Birch"><span class="toc-number">5.</span> <span class="toc-text">Birch</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#OPTICS"><span class="toc-number">6.</span> <span class="toc-text">OPTICS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">7.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/16/cplusplus-opencv/" title="c++ 中调用 opencv"><img src="/img/c18.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="c++ 中调用 opencv"/></a><div class="content"><a class="title" href="/2024/12/16/cplusplus-opencv/" title="c++ 中调用 opencv">c++ 中调用 opencv</a><time datetime="2024-12-16T09:33:13.000Z" title="发表于 2024-12-16 17:33:13">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/16/ubuntu-aptitude/" title="ubuntu 包管理工具 aptitude"><img src="/img/c17.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ubuntu 包管理工具 aptitude"/></a><div class="content"><a class="title" href="/2024/12/16/ubuntu-aptitude/" title="ubuntu 包管理工具 aptitude">ubuntu 包管理工具 aptitude</a><time datetime="2024-12-16T06:38:39.000Z" title="发表于 2024-12-16 14:38:39">2024-12-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/02/linux-transport-endpoint-is-not-connected/" title="transport endpoint is not connected"><img src="/img/c5.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="transport endpoint is not connected"/></a><div class="content"><a class="title" href="/2024/12/02/linux-transport-endpoint-is-not-connected/" title="transport endpoint is not connected">transport endpoint is not connected</a><time datetime="2024-12-02T05:37:05.000Z" title="发表于 2024-12-02 13:37:05">2024-12-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/29/python-compile-package-py/" title="python 编译或打包 py 文件"><img src="/img/c4.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python 编译或打包 py 文件"/></a><div class="content"><a class="title" href="/2024/11/29/python-compile-package-py/" title="python 编译或打包 py 文件">python 编译或打包 py 文件</a><time datetime="2024-11-29T09:11:28.000Z" title="发表于 2024-11-29 17:11:28">2024-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/11/11/python-mysql-db/" title="python 连接 mysql 数据库"><img src="/img/c24.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python 连接 mysql 数据库"/></a><div class="content"><a class="title" href="/2024/11/11/python-mysql-db/" title="python 连接 mysql 数据库">python 连接 mysql 数据库</a><time datetime="2024-11-11T05:40:54.000Z" title="发表于 2024-11-11 13:40:54">2024-11-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Jinzhong Xu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '695ca5d39e2ba2f9feb5',
      clientSecret: '9d4027af6364ff54595b7a8580977ec58c38a5ae',
      repo: 'xujinzh.github.io',
      owner: 'xujinzh',
      admin: ['xujinzh'],
      id: 'd4f6f5edbc860a22910c186075d603e0',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="众,妙,之,门" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>