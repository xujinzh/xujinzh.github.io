<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/letter-j.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/letter_j_favicon_32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/letter_j_favicon_16.ico">
  <link rel="mask-icon" href="/images/letter-j.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" integrity="sha256-jTIdiMuX/e3DGJUGwl3pKSxuc6YOuqtJYkM0bGQESA4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/orange/pace-theme-center-atom.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"xujinzh.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.10.1","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="本节介绍一些典型的聚类算法，如 K-Means, DBSCAN, 谱聚类，层次聚类，optics, birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，是类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量">
<meta property="og:type" content="article">
<meta property="og:title" content="几个聚类算法">
<meta property="og:url" content="https://xujinzh.github.io/2020/11/21/clustering-methods/index.html">
<meta property="og:site_name" content="J. Xu">
<meta property="og:description" content="本节介绍一些典型的聚类算法，如 K-Means, DBSCAN, 谱聚类，层次聚类，optics, birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，是类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png">
<meta property="og:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png">
<meta property="og:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png">
<meta property="og:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png">
<meta property="og:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png">
<meta property="article:published_time" content="2020-11-21T03:01:15.000Z">
<meta property="article:modified_time" content="2021-11-09T10:42:04.851Z">
<meta property="article:author" content="Jinzhong Xu">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="clustering">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png">


<link rel="canonical" href="https://xujinzh.github.io/2020/11/21/clustering-methods/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://xujinzh.github.io/2020/11/21/clustering-methods/","path":"2020/11/21/clustering-methods/","title":"几个聚类算法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>几个聚类算法 | J. Xu</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">J. Xu</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">215</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">20</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">255</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#K-Means"><span class="nav-number">1.</span> <span class="nav-text">K-Means</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DBSCAN"><span class="nav-number">2.</span> <span class="nav-text">DBSCAN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B0%B1%E8%81%9A%E7%B1%BB"><span class="nav-number">3.</span> <span class="nav-text">谱聚类</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">4.</span> <span class="nav-text">层次聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%9D%E8%81%9A%E5%BC%8F%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">4.1.</span> <span class="nav-text">凝聚式层次聚类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E8%A3%82%E5%BC%8F%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">4.2.</span> <span class="nav-text">分裂式层次聚类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Birch"><span class="nav-number">5.</span> <span class="nav-text">Birch</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OPTICS"><span class="nav-number">6.</span> <span class="nav-text">OPTICS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">7.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jinzhong Xu"
      src="/images/silence.jpg">
  <p class="site-author-name" itemprop="name">Jinzhong Xu</p>
  <div class="site-description" itemprop="description">众妙之门</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">255</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">215</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/xujinzh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xujinzh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jinzhongxu@csu.ac.cn" title="E-Mail → mailto:jinzhongxu@csu.ac.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/xjz333" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;xjz333" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://xujinzh.github.io/2020/11/21/clustering-methods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/silence.jpg">
      <meta itemprop="name" content="Jinzhong Xu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="J. Xu">
      <meta itemprop="description" content="众妙之门">
    </span>
    
    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="几个聚类算法 | J. Xu">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          几个聚类算法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-21 11:01:15" itemprop="dateCreated datePublished" datetime="2020-11-21T11:01:15+08:00">2020-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2021-11-09 18:42:04" itemprop="dateModified" datetime="2021-11-09T18:42:04+08:00">2021-11-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/research/" itemprop="url" rel="index"><span itemprop="name">research</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/research/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本节介绍一些典型的聚类算法，如 K-Means, DBSCAN, 谱聚类，层次聚类，optics, birch 等。聚类就是对大量未知标注的数据，按照内在相似性将其划分为多个类别，是类别内的数据相似度较大而类别间的数据相似度较小。通过计算每个类的代表点可以获得整个数据集的少量代表点，从而获得整个数据集的结构、形状信息。聚类算法通常处理无标签的数据集，因此常使用相似度或距离来处理。有衡量两个点（向量）直接的距离的，有衡量两个子集合之间的距离的，也有衡量点到子集合之间距离的。</p>
<a id="more"></a>

<p>首先先给出一个图，来直观看一下各个聚类算法的效果图。</p>
<p><img data-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png"></p>
<p>数学上，什么是距离？非空集合 $X$ 上的度量为一个函数（称之为“距离函数”或简称为“距离”）<br>$$<br>d: X \times X \to \mathbb{R}<br>$$<br>这里的 $\mathbb{R}$ 是实数集合，且对于所有 $X$ 内的 $x, y, z$，均满足如下条件：</p>
<ul>
<li><p>非负性，或分离公理<br>$$<br>d(x, y) \geq 0<br>$$</p>
</li>
<li><p>同一性，或同时公理<br>$$<br>d(x,y) = 0 \Longleftrightarrow x = y<br>$$</p>
</li>
<li><p>对称性<br>$$<br>d(x, y) = d(y, x)<br>$$</p>
</li>
<li><p>次加性，或三角不等式<br>$$<br>d(x, z) \leq d(x, y) + d(y, z)<br>$$<br>其次，给出几种计算相似度或距离的方法。</p>
</li>
</ul>
<ol>
<li><p>闵可夫斯基（或叫明氏）距离（Minkowski，欧式空间中的一种距离，$p$-范数距离，$p$不一定要是整数，但不可以小于1，不然三角不等式不成立）<br>$$<br>d(\vec{x}, \vec{y}) = (\sum_{i=1}^n |x_i - y_i|^p)^{\frac{1}{p}}<br>$$<br>当 $p = 1$ 时，称为曼哈顿距离（Manhattan，1-范数距离），$p = 2$ 时，称为欧式（欧几里得）距离（Euclidean，2-范数距离），$p = +\infty$ 时称为切比雪夫距离（Chebyshev，无穷范数距离）。</p>
</li>
<li><p>杰卡德（Jaccard）相似系数<br>$$<br>J(A, B) = \frac{|A \cap B|}{|A \cup B|}<br>$$</p>
</li>
<li><p>余弦相似度（cosine similarity）<br>$$<br>cos(\theta) = \frac{\vec{x}^T \vec{y}}{|\vec{x}| \cdot |\vec{y}|}<br>$$</p>
</li>
<li><p>Pearson 相似系数<br>$$<br>\rho_{\vec{x}\vec{y}} = \frac{cov(\vec{x}, \vec{y})}{\sigma_{\vec{x}}\sigma_{\vec{y}}}  = \frac{E[(\vec{x} - \mu_{\vec{x}})(\vec{y} - \mu_{\vec{y}})]}{\sigma_{\vec{x}}\sigma_{\vec{y}}}<br>$$</p>
</li>
<li><p>相对熵（K-L散度，K-L距离）<br>$$<br>D(p | q) = \sum_{\vec{x}} p(\vec{x}) \log{\frac{p(\vec{x})}{q(\vec{x})}} = E_{p(\vec{x})} \log{\frac{p(\vec{x})}{q(\vec{x})}}<br>$$</p>
</li>
<li><p>Hellinger 距离<br>$$<br>D_{\alpha}(p | q) = \frac{2}{1-\alpha^2}(1 - \int p(x)^{\frac{1+\alpha}{2}} q(x)^{\frac{1-\alpha}{2}} \mathrm{d}x)<br>$$</p>
</li>
</ol>
<h1 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h1><p>k-means 算法是使用最多，且相对简单的聚类算法。</p>
<p>k-means 算法，也被称为 k-平均或k-均值，是一些聚类算法的基础，如谱聚类。</p>
<p>假设输入样本为 $S = x_1, x_2, \cdots, x_m$，则算法步骤为</p>
<ol>
<li><p>选择初始的 $k$ 个类别中心 $\mu_1, \mu_2, \cdots, \mu_k$</p>
</li>
<li><p>对于每个样本 $x_i$，将其标记为距离类别中心最近的类别，即<br>$$<br>label_i = \underset{1 \leq j \leq k}{\arg \min} | x_j - \mu_j |<br>$$</p>
</li>
<li><p>将每个类别中心更新为隶属该类别的所有样本的均值<br>$$<br>\mu_j = \frac{1}{|c_j|} \underset{i \in c_j}{\sum} x_i<br>$$</p>
</li>
<li><p>重复最后两步，直到类别中心的变化小于某阈值。或者迭代次数达到某阈值，或者最小平方误差MSE（Minimum Squared Error）小于某阈值。</p>
</li>
</ol>
<p>在算法中，如果不取均值，改为取中位数，称为 k-mediods 聚类（k中值聚类）。</p>
<p>初始值的选择对 k-means 算法影响很大，通过 k-means++ 可以有效的选择初始聚类中心。具体的，</p>
<ol>
<li>首先任意选择一个初始样本点 $\mu_1 = x_1$ 为第一个聚类中心</li>
<li>其次，计算剩余点 $y_1, y_2, \cdots, y_m$ 到聚类中心 $\mu_1$ 的聚类，假设为 $d_1, d_2, \cdots, d_m$，那么依概率 $\frac{d_1}{d}, \frac{d_2}{d}, \cdots, \frac{d_m}{d}$ 来选择样本点 $y_1, y_2, \cdots, y_m$ 中的一个作为第二个聚类中心 $\mu_2$，其中 $d = \sum_{i=1}^m d_i$</li>
<li>重复步骤2，直到选到所需的 $k$ 个聚类中心 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>最后，使用 k-means 聚类算法在数据集 $x_1, x_2, \cdots, x_n$ 和  $k$ 个聚类中心 $\mu_1, \mu_2, \cdots, \mu_k$ 上进行迭代聚类</li>
</ol>
<p>关于簇数即聚类中心个数的选择，一种方法是遍历簇数取值 $1, 2, \cdots, n$，画图查看不同簇数取值下整个数据集 MSE 变化情况，即从数据集方差（当类簇数等于1）下降到0（当类簇数等于 $n$，即样本个数，每个样本聚成一个类），选择 MSE 变化的“肘关节点”；另一种方法是，当选择某个 $k$ 作为聚类中心树，进行 k-means 聚类，计算聚类后每个类簇的 MSE，对于取值较大 MSE 的类簇增加聚类中心个数，对于取值较小的，合并聚类中心。</p>
<p><font color='dd00dd'> k-means 深层解释 </font></p>
<p>假设样本集 ${ x_i }$  有 $k$ 个簇，每个簇样本数目为 $N_1, N_2, \cdots, N_k$，且每个簇都服从相同方差 $\sigma$ 的高斯分布，每个类簇的均值就是聚类中心 $\mu_i$，那么，k-means 聚类的最大似然函数是<br>$$<br>\Pi_{j=1}^k \Pi_{i=1}^{N_j} \frac{1}{\sqrt{2\pi}\sigma} \exp ^{-\frac{(x_i^{j} - \mu_j)}{2\sigma^2}}<br>$$<br>取对数后，只保留参数 $\mu_i$ 部分<br>$$<br>J(\mu_1, \mu_2, \cdots, \mu_k) = \frac{1}{2} \sum_{j=1}^k \sum_{i=1}^{N_j} (x_i^j - \mu_j)^2<br>$$<br>对函数 $J$ 关于参数变量 $\mu_1, \mu_2, \cdots, \mu_k$ 求偏导，得到<br>$$<br>\frac{\partial J}{\partial \mu_j} = - \sum_{i=1}^{N_j}(x_i^j - \mu_j) = 0 \Rightarrow \mu_j = \frac{1}{N_j} \sum_{i=1}^{N_j} x_i^j<br>$$<br>由此可以知道，k-means 运行的隐含假设是，每个类簇都服从高斯分布，且方差相同。即数据集服从方差相同的混合高斯分布。同时，该算法的目标函数是 MSE，且采用梯度下降算法进行迭代优化。因此，机器学习中非凸函数梯度下降算法的问题，该算法都会有，比如，初始点（初始聚类中心）的选择，局部最优问题，收敛震荡问题，鞍点问题等。同时，也提醒我们，可以使用 SGD 来求解 k-means（即 Mini-batch k-means 算法，不是把每个类的算有点个数 $N_j$ 都使用来计算均值，而是随机使用部分点 $N_j^{\prime} &lt; N_j$ 来计算均值），同样的，也可以采用 k-means++ 筛选初始点的方法来选择机器学习方法初始点。</p>
<p><font color='dd00dd'> k-means 聚类算法的优缺点 </font></p>
<p>优点：</p>
<ol>
<li>是解决聚类问题的一种经典算法，简单，快速</li>
<li>对处理大数据集，该算法保持可伸缩性和高效率</li>
<li>当簇近似分为高斯分布时，效果较好</li>
<li>可作为其他聚类方法的基础，如谱聚类</li>
</ol>
<p>缺点：</p>
<ol>
<li>在簇的平均值可被定义的情况下才能使用，可能不适应于某些应用</li>
<li>必须事先给出 $k$ 值（要生成的簇个数），而且对初值敏感，对于不同的初始值，可能会导致不同结果</li>
<li>不适合于发现非凸形状的簇或者大小差别很大的簇</li>
<li>对噪声和孤立点数据敏感</li>
</ol>
<p><strong>KMeans 算法的重要参数包括：n_clusters​.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">10</span>, <span class="number">2</span>], [<span class="number">10</span>, <span class="number">4</span>], [<span class="number">10</span>, <span class="number">0</span>]])</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit(X)</span><br><span class="line">kmeans.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], dtype=int32)</span><br><span class="line"></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"><span class="comment"># 聚类中心</span></span><br><span class="line">array([[<span class="number">10.</span>,  <span class="number">2.</span>],</span><br><span class="line">       [ <span class="number">1.</span>,  <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], dtype=int32)</span><br></pre></td></tr></table></figure>

<p><strong>MiniBatchKMeans 算法的重要参数包括：n_clusters, batch_size​.</strong></p>
<p>MiniBatchKMeans 一般会比 KMeans 结果差，但是运行更快。</p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"></span><br><span class="line">X = np.array(</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">-1</span>],</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">kmeans = MiniBatchKMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>, batch_size=<span class="number">6</span>, max_iter=<span class="number">10</span>).fit(X)</span><br><span class="line">kmeans.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)</span><br><span class="line"></span><br><span class="line">kmeans.cluster_centers_</span><br><span class="line"><span class="comment"># 聚类中心</span></span><br><span class="line">array([[<span class="number">3.95918367</span>, <span class="number">2.40816327</span>],</span><br><span class="line">       [<span class="number">1.12195122</span>, <span class="number">1.3902439</span> ]])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MiniBatchKMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>, batch_size=<span class="number">6</span>, max_iter=<span class="number">10</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], dtype=int32)</span><br></pre></td></tr></table></figure>



<h1 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）算法是一种基于密度的聚类算法。密度聚类方法的指导思想是，只要样本点的密度大于某阈值，则将该样本添加到最近的簇中。这类算法能克服基于距离的算法只能发现“类圆形”（凸）的聚类的缺点，可发现任意形状的聚类，且对噪声数据不敏感。但计算密度单元的计算复杂度大，需要建立空间索引来降低计算量。</p>
<p>DBSCAN 是一个比较有代表性的基于密度的聚类算法。与下面将要介绍的层次聚类方法不同，它将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在有“噪声”的数据中发现任意形状的聚类。</p>
<p>介绍 DBSCAN 需要了解如下几个概念：</p>
<ol>
<li>对象的 $\varepsilon$-邻域：给定对象在半径 $\varepsilon$ 内的区域。</li>
<li>核心对象：对于给定的数目 $m$，如果一个对象的 $\varepsilon$-邻域至少包含 $m$ 个对象，则称该对象为核心对象。</li>
<li>直接密度可达：给定一个对象集合 $D$，如果 $p$ 是在 $q$ 的 $\varepsilon$-邻域内，而 $q$ 是一个核心对象，我们说对象 $p$ 从对象 $q$ 出发是直接密度可达的。 </li>
<li>密度可达：如果存在一个对象链 $p_1, p_2, \cdots, p_n, p_1 = q, p_n = p$，对 $p_i \in D, (i \leq i \leq n)$,  $p_{i+1}$ 是从 $p_i$ 关于 $\varepsilon$ 和 $m$ 直接密度可达的，则对象 $p$ 是从对象 $q$ 关于 $\varepsilon$ 和 $m$ 密度可达的。</li>
<li>密度相连：如果对象集合 $D$ 中存在一个对象 $o$，使得对象 $p$ 和 $q$ 是从 $o$ 关于 $\varepsilon$ 和 $m$ 密度可达的，那么对象 $p$ 和 $q$ 是关于 $\varepsilon$ 和 $m$ 密度相连的。</li>
<li>簇：一个基于密度的簇是最大的密度相连对象的集合。</li>
<li>噪声：不包含在任何簇中的对象称为噪声。</li>
</ol>
<p>DBSCAN 算法流程：</p>
<ol>
<li>如果一个点 $p$ 的 $\varepsilon$-邻域包含多于 $m$ 个对象，则创建一个 $p$ 作为核心对象的新簇；</li>
<li>寻找并合并核心对象直接密度可达的对象；</li>
<li>没有新点可以更新簇时，算法结束。</li>
</ol>
<p>从算法流程可以看出，每个簇至少包含一个核心对象。非核心对象可以是簇的一部分，构成了簇的边缘。包含过少对象的簇被认为是噪声。DBSCAN 不需要事先给定聚类个数。对于大数据集因为要计算密度，不太适用。</p>
<p>DBSCAN 优势：</p>
<ol>
<li>不需要指定簇的个数</li>
<li>可以发现任意形状的簇</li>
<li>擅长找到离群点（检测任务）</li>
<li>需要的参数较少</li>
</ol>
<p>DBSCAN 劣势：</p>
<ol>
<li>高纬数据处理比较困难（可以先降维）</li>
<li>参数难以选择（参数对结果的影响非常大）</li>
<li>scikit-learn 中效率很慢（使用数据消减策略）</li>
</ol>
<p><strong>DBSCAN 算法的重要参数包括：$\varepsilon$=eps, m=min_samples​.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">8</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">25</span>, <span class="number">80</span>]])</span><br><span class="line">clustering = DBSCAN(eps=<span class="number">3</span>, min_samples=<span class="number">2</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DBSCAN(eps=<span class="number">3</span>, min_samples=<span class="number">2</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>



<h1 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h1><p>谱聚类通过计算矩阵的特征向量来实现。可以把谱聚类描述为 PCA + K-means.</p>
<p>谱聚类是一种基于图论的聚类方法，通过对样本数据的拉普拉斯矩阵的特征向量进行聚类，从而达到对样本数据聚类的目的。可以解决区域重叠问题。</p>
<p>谱分析的整体过程：</p>
<ol>
<li>给定一组数据 $x_1, x_2, \cdots, x_n$，记任意两个点之间的相似度（“距离” 的负函数）为 $s_{ij} = &lt;x_i, x_j&gt;$，形成相似度图：$G=(V, E)$. 如果 $x_i$ 和 $x_j$ 之间的相似度 $s_{ij}$ 大于一定的阈值，那么，两个点是连接的，权值记作 $s_{ij}$.</li>
<li>接下来，可以用相似度图来解决样本距离问题：找到图的一个划分，形成若干个组，使得不同组之间有较低的权值，组内有较高的权值。</li>
</ol>
<p><strong>谱聚类的流程：</strong></p>
<ol>
<li><p>计算任意两个样本之间的相似度，这里常采用高斯相似度 $w_{ij} = exp(-\frac{|x_i - x_j|^2}{2\sigma^2})$, 这里 $\sigma$ 是超参数；</p>
</li>
<li><p>构造相似度矩阵 $W = (w_{ij})$， 需要注意的是，主对角线元素本来是 $w_{ii} = 1$，这里替换成 0；</p>
</li>
<li><p>构造度矩阵 $D = diag(d_i)$，这里 $d_i = \sum_{i=1}^n w_{ij}$;</p>
</li>
<li><p>构造拉普拉斯矩阵 $L = D - W$，如二维的例子对应的拉普拉斯矩阵如下</p>
<p>$$D = \begin{bmatrix}<br>w_{12} + w_{13} &amp; 0 &amp; 0 \\<br>0 &amp; w_{21} + w_{23} &amp; 0 \\<br>0 &amp; 0 &amp; w_{31} + w_{32}<br>\end{bmatrix}$$  </p>
<p> $$W = \begin{bmatrix}<br>0 &amp; w_{12} &amp; w_{13} \\<br>w_{21} &amp; 0 &amp; w_{23} \\<br>w_{31} &amp; w_{32} &amp; 0<br>\end{bmatrix} $$ </p>
<p>$$L = \begin{bmatrix}<br> w_{12} + w_{13} &amp; -w_{12} &amp; -w_{13} \\<br> -w_{21} &amp; w_{21} + w_{23} &amp; -w_{23} \\<br> -w_{31} &amp; -w_{32} &amp; w_{31} + w_{32}<br> \end{bmatrix}$$</p>
</li>
</ol>
<ol start="5">
<li>求拉普拉斯矩阵的谱，即所有的特征向量，并从小到大进行排列，假设为 $\lambda_1, \lambda_2, \cdots, \lambda_k, \cdots, \lambda_n$, 其对应的特征向量分别为 $\mu_1, \mu_2, \cdots, \mu_k = (\mu_{ik})^T, \cdots, \mu_n$，其中，特征向量<br>$$<br>\begin{bmatrix}<br>\mu_{11} &amp; \mu_{12} &amp; \cdots &amp; \mu_{1k} &amp; \cdots &amp; \mu_{1n} \\<br>\mu_{21} &amp; \mu_{22} &amp; \cdots &amp; \mu_{2k} &amp; \cdots &amp; \mu_{2n} \\<br>\vdots   &amp; \vdots   &amp; \ddots &amp; \vdots   &amp; \ddots &amp; \vdots   \\<br>\mu_{n1} &amp; \mu_{n2} &amp; \cdots &amp; \mu_{nk} &amp; \cdots &amp; \mu_{nn}<br>\end{bmatrix}<br>$$</li>
</ol>
<p>   的每一行就是样本映射的特征。选取前 $k$ 列个特征向量（前 $k$ 小特征值对应的特征向量）作为主特征向量（类似 PCA），然后，对矩阵<br>   $$<br>   \begin{bmatrix}<br>   \mu_{11} &amp; \mu_{12} &amp; \cdots &amp; \mu_{1k} \\<br>   \mu_{21} &amp; \mu_{22} &amp; \cdots &amp; \mu_{2k} \\<br>   \vdots   &amp; \vdots   &amp; \ddots &amp; \vdots   \\<br>   \mu_{n1} &amp; \mu_{n2} &amp; \cdots &amp; \mu_{nk}<br>   \end{bmatrix}<br>   $$<br>   按行聚类，采用聚类方法 k-means，这样就得到了原始样本的谱聚类。</p>
<p><strong>证明，拉普拉斯矩阵 $L = D - W$ 是半正定矩阵。</strong></p>
<p>对任意非零向量 $f$<br>$$<br>f^T L f = f^T D f - f^T W f = \sum_{i=1}^n d_i f_i^2 - \sum_{i,j=1}^n f_i f_j w_{ij} \\<br>= \frac{1}{2} (\sum_{i=1}^n d_i f_i^2 - 2 \sum_{i,j=1}^n f_i f_j w_{ij} + \sum_{j=1}^n d_j f_j^2) \\<br>= \frac{1}{2} \sum_{i,j=1}^n w_{ij}(f_i - f_j)^2 \geq 0.<br>$$<br>$L$ 是对称半正定矩阵，最小特征值是0， 相应的特征向量是全1向量。其他特征值大于零。</p>
<p>它适用于少数群集，但不建议用于许多群集。</p>
<p><font color='dd0000'>谱聚类算法：未正则拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i=1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li>计算拉普拉斯矩阵 $L = D - W$</li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i = 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i=1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i = {j|y_j \in C_i}$</li>
</ul>
<p><font color='dd0000'>谱聚类算法：随机游走拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i=1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li><font color='dd00dd'>计算拉普拉斯矩阵 $L_{rw} = D^{-1}(D - W)$</font></li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i = 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i=1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i = {j|y_j \in C_i}$</li>
</ul>
<p><font color='dd0000'>谱聚类算法：对称拉普拉斯矩阵</font></p>
<p>输入：$n$ 个点 ${x_i}_{i=1, 2, \cdots, n}$，簇的数目 $k$</p>
<ul>
<li>计算 $n \times n$ 的相似度矩阵 $W$ 和度矩阵 $D$</li>
<li><font color='dd00dd'>计算拉普拉斯矩阵 $L_{sym} = D^{-1/2}(D - W)D^{-1/2}$</font></li>
<li>计算 $L$ 的前 $k$ 个特征向量 $\mu_1, \mu_2, \cdots, \mu_k$</li>
<li>将 $k$ 个列向量 $\mu_1, \mu_2, \cdots, \mu_k$ 组成矩阵 $U \in \mathbb{R}^{n \times k}$</li>
<li>对于 $i = 1, 2, \cdots, n$， 令 $y_i \in \mathbb{R}^k$ 是 $U$ 的第 $i$ 行的向量</li>
<li><font color='dd00dd'>对于 $i=1,2,\cdots, n$，将$y_i \in \mathbb{R}^k $ 依次单位化，使得 $|y_i| = 1$ </font></li>
<li>使用 <strong>k-means</strong> 算法将点 ${y_i}_{i=1, 2, \cdots, n}$ 聚类成簇 $C_1, C_2, \cdots, C_k$</li>
<li>输出簇 $A_1, A_2, \cdots, A_k$，其中，$A_i = {j|y_j \in C_i}$</li>
</ul>
<p><strong>有时候，也会使用拉普拉斯矩阵 $L = W - D$，或者 $L = D^{-1}W$ (因为 $L = D^{-1}(D-W) = I - D^{-1}W$，不考虑单位矩阵效果一样)，此时，需要对特征值进行从大到小排列，选取相应的特征向量。</strong></p>
<p><em>当未给定先验知识的情况下，优先选择随机游走拉普拉斯矩阵。对于 $n$ 个样本，通过相似度构建一个相似度连通图，共有 $n(n+1)/2$ 条边，边上的权重就是两个样本之间的相似度。那么谱聚类的目标函数就是使用剪刀剪断一个边使得损失值最小（即特征值）。</em></p>
<p>随机游走和拉普拉斯矩阵的关系（当拉普拉斯矩阵 $L = D^{-1}W$ 时）</p>
<p>图论中的随机游走是一个随机过程，它从一个顶点跳转到另外一个顶点。谱聚类即找到图的一个划分，使得随机游走在相同的簇中停留而几乎不会游走到其他簇。转移矩阵是 $P=D^{-1}W$，表示从顶点 $v_i$ 跳转到顶点 $v_j$ 的概率正比于边的权值，即 $p_{ij} = w_{ij}/d_i$.</p>
<p>进一步的，可以考虑：</p>
<ul>
<li>谱聚类中的 $k$ 如何确定？$k^{\star} = \arg \max_k |\lambda_{k+1} - \lambda_k|$</li>
<li>最后一步的 $k-means$ 的作用是什么？目标函数是关于子图划分指示向量的函数，该向量的值根据子图划分确定，是离散的。该问题是 NP 的，转化成求连续实数域上的解，最后用 $k-means$ 算法离散化</li>
<li>未正则、对称、随机游走拉普拉斯矩阵，首先哪个？随机游走拉普拉斯矩阵</li>
<li>谱聚类可以用切割图，随机游走，扰动论等解释</li>
</ul>
<p><strong>谱聚类算法的重要参数包括：n_clusters, assign_labels, eigen_solver​.</strong></p>
<p>这里 n_cluster 表示聚类个数，也表示投影子空间的维数。eigen_solver 表示矩阵分解求特征值的策略，可选取值有 $None, arpack, lobpcg, or\ amg$, 其中， $amg$ 需要安装 $pyamg$ 才能运行，该分解策略能够有效提高大数据集、稀疏问题的计算速度，但可能会导致结果的不稳定。assign_labels​ 用于在嵌入空间中分配标签的策略。拉普拉斯嵌入后，有两种分配标签的方法。可以应用 <strong>kmeans</strong>，它是一种流行的选择。但是它也可能对初始化敏感。<strong>discretize</strong> 是另一种对随机初始化不太敏感的方法。</p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> SpectralClustering</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">7</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line">clustering = SpectralClustering(</span><br><span class="line">    n_clusters=<span class="number">2</span>, assign_labels=<span class="string">&quot;discretize&quot;</span>, random_state=<span class="number">0</span></span><br><span class="line">).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 聚类结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>也可以直接返回聚类结果，不显示调用 clustering.labels_ </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SpectralClustering(</span><br><span class="line">    n_clusters=<span class="number">2</span>, assign_labels=<span class="string">&quot;discretize&quot;</span>, random_state=<span class="number">0</span></span><br><span class="line">).fit_predict(X)</span><br><span class="line"><span class="comment"># 直接打印出来聚类结果</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>



<h1 id="层次聚类"><a href="#层次聚类" class="headerlink" title="层次聚类"></a>层次聚类</h1><p>层次聚类（Hierarchical clustering）方法对给定的数据集进行层次分解，直到满足某种条件为止。具体地，可分为凝聚式层次聚类和分裂式层次聚类。</p>
<h2 id="凝聚式层次聚类"><a href="#凝聚式层次聚类" class="headerlink" title="凝聚式层次聚类"></a>凝聚式层次聚类</h2><p>凝聚式层次聚类又叫作 AGNES 算法。该算法属于层次聚类常采用的算法。相对于下面分裂式层次聚类，该算法复杂度更小。</p>
<p>scikit-learn 中实现的层次聚类算法 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering">AgglomerativeClustering</a>, 使用自底向上执行层次聚类。每个样本都从自己聚类开始，并且聚类被连续合并在一起。合并或链接的标准成为算法的关键点。实现的合并策略的链接标准有 Ward （最小化所有类平方差和，一种方差最小化方法，某种意义上与k -means相似）、Maximum or complete linkage（最小化两对类簇的最大距离）、Average linkage（最小化两对类簇所有观测样本的距离平均值）、Single linkage（最小化两对类簇之间的最小观测样本距离）。不同策略得到的层次聚类效果图如下：</p>
<p><img data-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png"></p>
<ol>
<li>最小距离<ul>
<li>两个集合中最近的两个样本的聚类，single</li>
<li>容易形成链状结构</li>
</ul>
</li>
<li>最大距离<ul>
<li>两个集合中最远的两个样本的距离，maximum or complete</li>
<li>若存在异常值则不稳定</li>
</ul>
</li>
<li>平均距离<ul>
<li>两个集合中样本间两两距离的平均值，average</li>
<li>两个集合中样本间两两距离的平方和，ward</li>
</ul>
</li>
</ol>
<p>注意，Agglomerative 聚类具有一种 “富人更富有” 的行为，这将导致聚类大小不均匀。在这方面，single linkage 策略是最糟糕的，ward 能够给出最公正的大小。但是，ward 无法更改相似性（或者距离），因此对于非欧度量，average linkage 是一个更好的选择。single linkage 虽然对噪声敏感，但是计算效率高，能够推广到大数据集上。同时，在非球形数据上表现良好。</p>
<p>层次聚类能够给出数据结构的可视化信息，如类似下面的树状图：</p>
<p><img data-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png"></p>
<p>当与连接矩阵一起使用时，AgglomerativeClustering 也可以扩展到大量样本，但是当在样本之间不添加连接限制时，计算开销很大，因为它在每一步都考虑了所有可能的合并。</p>
<p>AgglomerativeClustering 有趣的方面是，可以通过一个连通性矩阵将连通性约束添加到该算法中（只能将相邻的聚类合并在一起），该矩阵为每个样本定义遵循给定数据结构的相邻样本。例如，在下面的瑞士卷示例中，连接性约束禁止合并瑞士卷上不相邻的点，因此避免形成在卷的重叠折叠部分延伸的簇。</p>
<p><img data-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_001.png"></p>
<p><img data-src="https://scikit-learn.org/stable/_images/sphx_glr_plot_ward_structured_vs_unstructured_002.png"></p>
<p>这些约束对于强加特定的局部结构很有用，同时它们也使算法更快，尤其是在样本数量很多时。</p>
<p><strong>层次聚类（这里指 Agglomerative Clustering）的重要参数：n_clusters, linkage.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line">clustering = AgglomerativeClustering(n_clusters=<span class="number">2</span>, linkage=<span class="string">&#x27;ward&#x27;</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">AgglomerativeClustering(n_clusters=<span class="number">2</span>, linkage=<span class="string">&#x27;ward&#x27;</span>).fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h2 id="分裂式层次聚类"><a href="#分裂式层次聚类" class="headerlink" title="分裂式层次聚类"></a>分裂式层次聚类</h2><p>分裂式层次聚类是 DIANA 算法。采用自顶向下的策略，它首先将所有对象置于一个簇中，然后逐渐细分为越来越小的簇，直到达到了某个终结条件。</p>
<h1 id="Birch"><a href="#Birch" class="headerlink" title="Birch"></a>Birch</h1><p>Birch 算法叫做平衡迭代削剪聚类算法，它是对层次聚类算法的一种优化，能够处理大数据集。</p>
<p>Birch 使用三元组的聚类特征表示类簇。通过构建满足分支因子和簇直径限制的聚类特征树（Clustering Feature Tree，CFT）来求聚类，数据本质上是有损压缩到一组“群集特征”节点（CF节点）的。聚类特征树其实是一个具有两个参数<font color='dd0000'>分支因子</font>和<font color='dd0000'>类直径</font>的高度平衡树。分支因子规定了树的每个节点的子女的最多个数，而类直径体现了对这一类点的距离范围。非叶子节点为它的子女的最大特征值。聚类特征树的构建是一个动态过程，可以随时根据数据对模型进行更新操作。</p>
<p>假设一个类簇包含的样本点集合为 ${x_1, x_2, \cdots, x_n}$ ，三元组存储的信息分别是 $(n, s_1, s_2)$，其中 $n$ 表示样本点个数，$s_1 = x_1 + x_2 + \cdots + x_n$，$s_2 = x_1 \cdot x_1 + x_2 \cdot x_2 + \cdots + x_n \cdot x_n$. 根据这个三元组信息，类直径和分支因子可以做如下事情。</p>
<p>类直径：判断样本是否属于当前簇</p>
<p>分支因子：当一个节点的子节点数目超过分支因子的时候，将该节点划分为两个子节点。当一个叶子节点的样本数目超过分支因子的时候，将叶子节点分成两个叶子节点。</p>
<p>优点：</p>
<ol>
<li>适合大规模数据集，线性效率</li>
<li>节约内存，所有样本都在磁盘上，CFT 仅仅存储 CF 节点和对应的指针</li>
<li>可以识别噪声。还可以对数据集进行初步预处理，作为其他聚类算法的输入，如层次聚类</li>
</ol>
<p>缺点：</p>
<ol>
<li>只适合分布呈凸形或者球形的数据集，需要给定聚类个数和簇之间的相关参数</li>
<li>无法很好地缩放到高维数据。根据经验，如果特征维度大于20，通常最好使用 MiniBatchKMeans</li>
<li>由于 CFT 对每个节点的 CF 个数有限制，导致聚类结果可能和真实类别分布不同</li>
</ol>
<p><strong>Birch 算法的重要参数：threshold, branching_factor, n_clusters.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> Birch</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0.3</span>, <span class="number">1</span>], [<span class="number">-0.3</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">-1</span>], [<span class="number">0.3</span>, <span class="number">-1</span>], [<span class="number">-0.3</span>, <span class="number">-1</span>]]</span><br><span class="line">brc = Birch(n_clusters=<span class="literal">None</span>)</span><br><span class="line">brc.fit(X)</span><br><span class="line">brc.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">brc.fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>参考链接</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6179132.html">BIRCH聚类算法原理</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54837341">数据挖掘入门笔记——BIRCH聚类（一拍即合）</a></li>
</ol>
<h1 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h1><p>OPTICS(Ordering Points To Identify the Clustering Structure)聚类算法是前面 DBSCAN 算法的一种扩展，能够解决 DBSCAN 对超参数敏感（不易选择超参数）的问题。与 DBSCAN 相比，OPTICS 算法的改进主要在于对输入参数不敏感。</p>
<p>OPTICS 算法不显示地生成数据聚类，它只是对数据对象集合中的对象进行排序，得到一个有序的对象列表，其中包含了足够的信息用来提取聚类。</p>
<p>因为 OPTICS 算法是 DBSCAN 算法发发的改进，因此很多概念可以沿用，如（直接）密度可达，密度相连，核心点，半径参数 $\varepsilon$，最小点数 $M$ 等。假设数据集 $X = {x_1, x_2, \cdots, x_n }$，</p>
<p><font color='dd00dd'>核心距离（core distance）</font></p>
<p>点$x \in X$ 成为核心点的最小邻域半径定义为 $x$ 的核心距离，<br>$$<br>cd(x) =<br>\begin{cases}<br>undefined， &amp; |N_{\varepsilon}(x)|&lt; M \\<br>d(x, y(N_{\varepsilon}^M, x)) &amp; |N_{\varepsilon}(x)| \geq M<br>\end{cases}<br>$$<br>其中 $y(N_{\varepsilon}^i, x)$ 表示集合 $N_{\varepsilon}(x)$ 中与点 $x$ 第 $i$ 近邻的点。</p>
<p>从定义可以知道，如果 $x$ 是核心点，那么核心距离 $0 \leq cd(x) \leq \varepsilon$.</p>
<p><font color='dd00dd'>可达距离（reachability distance）</font></p>
<p>对于点 $x, y \in X$，$y$ 关于 $x$ 的可达距离定义为<br>$$<br>rd(y, x) =<br>\begin{cases}<br>undefined, &amp; |N_{\varepsilon}(x)| &lt; M \\<br>\max { cd(x), d(x, y) } &amp; |N_{\varepsilon}(x)| \geq M<br>\end{cases}<br>$$<br>即 $rd(y, x)$ 表示使得 $x$ 为核心点且 $y$ 从 $x$ 直接密度可达同时成立的最小邻域半径。</p>
<p>通俗理解就是，对于核心点 $x$ ，与点 $x$ 距离小于等于核心距离的可达距离调整为核心距离，大于核心距离的可达距离就是两点之间的距离。</p>
<p>可达距离 $rd(y, x)$ 的值与点 $y$ 所在空间的密度有关，密度越大，它从相邻节点直接密度可达的距离就越小。如果聚类时想要朝着数据尽量稠密的空间进行扩展，那么可达距离最小的点是最佳的选择。为此，OPTICS 算法中用一个可达距离<strong>升序排列</strong>的有序种子队列存储带扩展的点，以迅速定位稠密空间的数据对象。</p>
<p>在给出 OPTICS 算法之前，先引入以下定义</p>
<ol>
<li>$p_i, i = 1, 2, \cdots, N$：OPTICS 算法的输出序数组，$p_i \in { 1, 2, \cdots, N }$ 表示排在第 $i$ 个位置的节点的编号</li>
<li>$r_i, i = 1, 2, \cdots, N$：第 $i$ 号节点的可达距离</li>
<li>$c_i, i = 1, 2, \cdots, N$：第 $i$ 号节点的核心距离</li>
<li>$v_i, i = 1, 2, \cdots, N$：表示节点便函是否被访问过的辅助数组，$0$ 表示未访问过，$1$ 表示访问过。这里是否被访问过是指是否被加入到输出序数组 $p$ 中</li>
</ol>
<p>OPTICS 算法：</p>
<ul>
<li><p>初始化</p>
<ol>
<li>给定参数 $\varepsilon, M$</li>
<li>生成 $N_{\varepsilon}(i), i = 1, 2, \cdots, N$</li>
<li>生成 $c_i, i = 1, 2, \cdots, N$</li>
<li>令 $v_i = 0, i = 1, 2, \cdots, N$</li>
<li>令 $r_i = undefined, i = 1, 2, \cdots, N$</li>
<li>令 $k = 1, I = { 1, 2, \cdots, N }$</li>
<li>将队列 seedlist 初始化为空</li>
</ol>
</li>
<li><p>主流程</p>
<p>while ($I \neq 0$)</p>
<p>{</p>
<p>​    从 $I$ 中任取一个元素 $i$，令 $I:= I \ { i }$</p>
<p>​    if ($v_i = 0$)  # 表示 $i$ 号节点还没有被处理过</p>
<p>​    {</p>
<p>​        （1）令 $v_i = 1$</p>
<p>​        （2）令 $p_k = i, k = k + 1$</p>
<p>​        （3）若 $|N_{\varepsilon}(i)| \geq M$ (即 $i$ 为核心点)，则</p>
<p>​            （3.1）调用 insertlist($N_{\varepsilon}(i), {v_i}_{i=1}^N, {r_i}_{i=1}^N, c_i$, seedlist)，将 $N_{\varepsilon}(i)$ 中未被访问过的节点按照可达距离插入到队列 seedlist 中。（初始化 seedlist）</p>
<p>​            （3.2）while (seedlist not empty)</p>
<p>​                        {</p>
<p>​                            （a）从 seedlist 中取出<strong>第一个元素</strong> $j$ （其可达距离值最小）</p>
<p>​                            （b）令 $v_j = 1$</p>
<p>​                            （c）令 $p_k = j, k = k + 1$</p>
<p>​                            （d）若 $|N_{\varepsilon}(j)| \geq M$ （即 $j$ 为核心点），则调用 insertlist($N_{\varepsilon}(j), {v_l}_{l=1}^N, {r_l}_{l=1}^N, c_j$, seedlist)，将 $N_{\varepsilon}(j)$ 中未被访问过的节点按照可达距离插入到队列 seedlist 中</p>
<p>​                        }</p>
<p>​    }</p>
</li>
</ul>
<p>  }</p>
<ul>
<li><p>insertlist($N_{\varepsilon}(j), {v_l}_{l=1}^N, {r_l}_{l=1}^N, c_j$, seedlist) 模块的算法</p>
<p>for all $J \in N_{\varepsilon}(K)$ do</p>
<p>​    if ($v_J = 0$)</p>
<p>​        $rd = \max { cd_K, d(x_K, x_J)}$</p>
<p>​        if ($r_J = undefined$)</p>
<ol>
<li><p>令 $r_j = rd$</p>
</li>
<li><p>将节点 $J$ 按照可达距离值插入到队列 seedlist 的适当位置</p>
<p>else</p>
<p>if ($rd &lt; r_J$)</p>
<ol>
<li>令 $r_J = rd$</li>
<li>将节点 $J$ 按照可达距离值插入到队列 seedlist 的适当位置</li>
</ol>
</li>
</ol>
</li>
<li><p>得到数据 ${ p_i }_{i=1}^N, { c_i }_{i=1}^N, { r_i }_{i=1}^N$ 后，就可以利用它们进行聚类。此时还需要一个邻域半径参数 $\lambda \leq \varepsilon$</p>
</li>
<li><p>与 DBSCAN 类似，引入 cluster 标记数组<br>$$<br>m_i =<br>\begin{cases}<br>j(&gt;0), &amp;  x_i 属于第 j 个 cluster \\<br>-1, &amp; x_i 为噪声<br>\end{cases}<br>$$<br>所以，要得到一个聚类，只需要生成标记数组 ${ m_i }_{i=1}^N$ 即可</p>
</li>
<li><p>生成标记数组（ OPTICS Cluster extracting）</p>
<p>clusterID = -1</p>
<p>$k = 1$</p>
<p>for $i = 1, 2, \cdots, N$, do</p>
<p>​    $j = p_i$</p>
<p>​    if ($r_j &gt; \lambda$ or $r_j = undefined$)</p>
<p>​        if ($c_j \neq undefinde$ and $c_j \leq \lambda$)</p>
<p>​            clusterID = $k$</p>
<p>​            $k = k + 1$</p>
<p>​            $m_j = $ clusterID</p>
<p>​        else</p>
<p>​            $m_j = -1$</p>
<p>else</p>
<p>​    $m_j = $ clusterID</p>
</li>
</ul>
<p><strong>OPTICS 算法的重要参数：min_samples.</strong></p>
<p>python 代码示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> OPTICS</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">7</span>, <span class="number">3</span>]])</span><br><span class="line">clustering = OPTICS(min_samples=<span class="number">2</span>).fit(X)</span><br><span class="line">clustering.labels_</span><br><span class="line"><span class="comment"># 运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接返回聚类结果，不需要调用 clustering.labels_</span></span><br><span class="line">OPTICS(min_samples=<span class="number">2</span>).fit_predict(X)</span><br><span class="line"><span class="comment">#运行结果如下</span></span><br><span class="line">array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>参考链接</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/itplus/article/details/10089323">聚类算法初探（六）OPTICS</a></li>
<li><a target="_blank" rel="noopener" href="https://www.biaodianfu.com/optics.html">密度聚类算法之OPTICS</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/41930932">OPTICS聚类算法</a></li>
</ol>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/clustering.html">Scikit-learn Clustering</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>感谢支持！</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="Jinzhong Xu 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Jinzhong Xu 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>原作者： </strong>Jinzhong Xu
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://xujinzh.github.io/2020/11/21/clustering-methods/" title="几个聚类算法">https://xujinzh.github.io/2020/11/21/clustering-methods/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"><i class="fa fa-tag"></i> machine learning</a>
              <a href="/tags/clustering/" rel="tag"><i class="fa fa-tag"></i> clustering</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/19/cloud-by-filebrowser-and-nginx/" rel="prev" title="使用 filebrowser 和 Nginx 在 VPS 搭建云盘">
                  <i class="fa fa-chevron-left"></i> 使用 filebrowser 和 Nginx 在 VPS 搭建云盘
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/11/24/markdown-matrix/" rel="next" title="在 markdown 中书写矩阵">
                  在 markdown 中书写矩阵 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jinzhong Xu</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">541k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">8:12</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script size="300" alpha="0.6" zIndex="-1" src="https://cdn.jsdelivr.net/npm/ribbon.js@1.0.2/dist/ribbon.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/lozad@1.16.0/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>



  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"xujinzh","repo":"xujinzh.github.io","client_id":"695ca5d39e2ba2f9feb5","client_secret":"9d4027af6364ff54595b7a8580977ec58c38a5ae","admin_user":"xujinzh","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"d4f6f5edbc860a22910c186075d603e0"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
