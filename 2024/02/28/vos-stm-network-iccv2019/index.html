<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Video Object Segmentation using Space-Time Memory Networks | J. Xu</title><meta name="author" content="Jinzhong Xu"><meta name="copyright" content="Jinzhong Xu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇介绍一篇发表在 ICCV 2019 上的半监督视频目标分割文章：Video Object Segmentation using Space-Time Memory Networks. 将历史帧及 MASK 作为额外的记忆指导当前帧的跟踪与分割。">
<meta property="og:type" content="article">
<meta property="og:title" content="Video Object Segmentation using Space-Time Memory Networks">
<meta property="og:url" content="https://xujinzh.github.io/2024/02/28/vos-stm-network-iccv2019/index.html">
<meta property="og:site_name" content="J. Xu">
<meta property="og:description" content="本篇介绍一篇发表在 ICCV 2019 上的半监督视频目标分割文章：Video Object Segmentation using Space-Time Memory Networks. 将历史帧及 MASK 作为额外的记忆指导当前帧的跟踪与分割。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xujinzh.github.io/img/c30.jpg">
<meta property="article:published_time" content="2024-02-28T03:20:20.000Z">
<meta property="article:modified_time" content="2024-03-05T08:19:32.248Z">
<meta property="article:author" content="Jinzhong Xu">
<meta property="article:tag" content="video">
<meta property="article:tag" content="vos">
<meta property="article:tag" content="STM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xujinzh.github.io/img/c30.jpg"><link rel="shortcut icon" href="/img/letter-j.png"><link rel="canonical" href="https://xujinzh.github.io/2024/02/28/vos-stm-network-iccv2019/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Video Object Segmentation using Space-Time Memory Networks',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-05 16:19:32'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/silence.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">405</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">316</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="J. Xu"><img class="site-icon" src="/img/letter-j.png"/><span class="site-name">J. Xu</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Video Object Segmentation using Space-Time Memory Networks</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-28T03:20:20.000Z" title="发表于 2024-02-28 11:20:20">2024-02-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-05T08:19:32.248Z" title="更新于 2024-03-05 16:19:32">2024-03-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/research/">research</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/research/video-object-segmentation/">video object segmentation</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Video Object Segmentation using Space-Time Memory Networks"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>本篇介绍一篇发表在 ICCV 2019 上的半监督视频目标分割文章：Video Object Segmentation using Space-Time Memory Networks. 将历史帧及 MASK 作为额外的记忆指导当前帧的跟踪与分割。</p>
<span id="more"></span>


<h1 id="文章资源"><a href="#文章资源" class="headerlink" title="文章资源"></a>文章资源</h1><ul>
<li>预印本 (Preprint)：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00607">arxiv</a></li>
<li>正式发表版本 (Version of Record, VOR)：<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.pdf">ICCV 2019</a></li>
<li>代码仓库：<a target="_blank" rel="noopener" href="https://github.com/seoungwugoh/STM">github</a></li>
</ul>
<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>文章题目是 Video Object Segmentation using Space-Time Memory Networks，指明文章的应用领域是视频目标分割(Video Object Segmentation, VOS)，文章使用自己提出的时空记忆网络(STM, Space-Time Memory Networks)来解决视频目标分割(VOS)任务。</p>
<h1 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h1><p>作者分别是 Seoung Wug Oh, Joon-Young Lee, Ning Xu, Seon Joo Kim，其中第一作者是 Seoung Wug Oh，他来自于韩国首尔的基督新教私立研究型大学，延世大学（Yonsei University, 1885），该工作是其在 Adobe Research 实习时所做。同时，Seon Joo Kim 也来自该大学。</p>
<p>其他两位作者 Joon-Young Lee, Ning Xu 来自于 Adobe Research.</p>
<h1 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h1><p>文章把视频分割任务看作是分离视频中所有帧的前景和背景像素问题。文章要解决的是一类半监督视频分割任务，即首帧的目标 MASK 已经给定，预测后续所有帧中目标的 MASKs。视频目标分割任务非常具有挑战性，因为随着时间增长目标的形状很可能会发生剧烈的改变，同时也可能会出现遮挡、漂移（分割到错误的目标上）等情况。</p>
<h1 id="算法关键点示意图"><a href="#算法关键点示意图" class="headerlink" title="算法关键点示意图"></a>算法关键点示意图</h1><p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm.png?raw=true"></p>
<p>以往的目标跟踪算法是使用前一帧、首帧、首帧+前一帧等进行指导当前帧的跟踪与分割，浪费了丰富的历史帧资源。STM 采用所有历史帧（包含MASK）来指导当前帧的跟踪与分割。</p>
<p>在 STM 框架中，历史帧和 MASK 形成了额外的记忆，当前帧作为一个 query 从记忆的 MASK 中进行分割，具体地，query 和记忆在特征空间中进行稠密的匹配，覆盖所有时空像素位置。这能够很好的处理目标形状改变和遮挡问题。</p>
<h1 id="想法引出"><a href="#想法引出" class="headerlink" title="想法引出"></a>想法引出</h1><p>一些视频目标分割算法(作者称为传播的方法)尝试从前一帧的 MASK 来预测当前帧的目标 MASK，这种方法能够很好的解决目标形状的改变问题，但缺乏对遮挡和错误漂移的健壮性。另一些方法（作者称为检查的方法）单纯的使用第一帧的 MASK 来预测后续每一帧的目标 MASK，这种方法和前面的方法真好形成对比，虽然对遮挡和错误漂移具有一定的健壮性，但是不能很好的解决目标形变问题。一些研究者采用首帧和前一帧作为参考帧，提供更多的线索来预测当前帧的目标 MASK，这种方法表现出良好的准确性，是目前最好的方法（作者称为混合方法，混合传播与检测）。</p>
<p>既然使用更多帧（首帧和前一帧）对于视频目标分割有益，那么利用再多一些帧（可能是视频所有帧）是否会更好。文章基于此想法设计一个有效的 DNN 框架利用所有（已预测的）帧的信息解决当前帧目标 MASK 预测问题。提出基于记忆网络的新颖的 DNN 系统，该系统能够对当前帧（query frame）的所有像素计算它与视频多帧的像素的视频注意力（spatio-temporal attention），来预测当前帧每一个像素是否属于前景。该网络称为 STM (Spatio-Temporal Memory) 网络，它利用了丰富的参考帧信息，能够更好的处理目标形状改变、遮挡和漂移等问题。</p>
<h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h2 id="半监督视频目标分割"><a href="#半监督视频目标分割" class="headerlink" title="半监督视频目标分割"></a>半监督视频目标分割</h2><ul>
<li>基于传播的方法。如上图1（a），使用前一帧预测下一帧。</li>
<li>基于检测的方法。如上图1（b），使用首帧预测当前帧。</li>
<li>基于混合的方法，混合检测和传播方法。如图1（c）.</li>
<li>在线学习方法。测试时使用初始帧的 MASK 微调网络，虽然能够提高准确性，但是推理时计算昂贵。</li>
<li>离线学习方法。常常设计一个网络能够处理推理时（不可知的）目标分割任务。</li>
</ul>
<p>文章提出的 STM 属于离线学习方法。通过在额外记忆中维护中间结果（而不是固定帧数），能够自适应的选择必要的信息进行跟踪与分割。</p>
<h2 id="记忆网络"><a href="#记忆网络" class="headerlink" title="记忆网络"></a>记忆网络</h2><p>记忆网络是指具有外部存储器的神经网络，其中信息可以按目的写入和读取。可以端到端训练的记忆网络最早是在 NLP 研究中提出的，用于文献问答。通常在这些方法中，可记忆信息分别嵌入到键(输入)和值(输出)特征向量中。键用于寻址相关的记忆，其对应的值被返回。近年来，记忆网络已被应用于个性化图像字幕、视觉跟踪、电影理解和摘要等视觉问题。</p>
<p>虽然文章是基于记忆网络，但扩展了记忆网络的思想，使其适合半监督视频对象分割任务。显然，带有对象掩码的帧被放到记忆中，要分割的帧充当查询。记忆是动态更新的新预测掩模，它无需在线学习。</p>
<p>目标是在给定一组带注释的帧作为记忆的情况下进行逐像素预测。因此，查询帧中的每个像素需要在不同的时空位置访问存储帧中的信息。为此，将记忆转化为包含像素级信息的四维张量，并提出时空记忆读取操作，从四维记忆中定位并读取相关信息。从概念上讲，记忆读取可以被认为是一种时空注意力算法，因为计算每个查询像素的关注时间和地点，以确定像素是否属于前景对象。</p>
<h1 id="Space-Time-Memory-Networks-STM"><a href="#Space-Time-Memory-Networks-STM" class="headerlink" title="Space-Time Memory Networks(STM)"></a>Space-Time Memory Networks(STM)</h1><p>在 STM 框架中，视频帧使用第一帧中给出的 ground truth 注释从第二帧开始顺序处理。在视频处理过程中，将带有对象掩码的过去帧(在第一帧给出或在其他帧估计)作为记忆帧，将没有对象掩码的当前帧作为查询帧。框架概述如下图所示。</p>
<p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/STM-network.png?raw=true"></p>
<p>当前帧（query）和记忆帧（memory，过去所有帧和 MASK）都通过一个深度 encoder 编码为 key 和 value 特征对。对于当前帧，encoder 的输入仅仅是当前帧图像，而对于记忆帧，encoder 的输入是过去帧的图像和对应的目标 MASK. 但是，输出都是 key 和 value 特征向量对，其中，key 向量用于寻址（addressing）。具体地，query 帧的 key 向量分别于 memory 帧的 key 向量计算相似度，利用该相似度绝对 when-and-where 去检索相关的 memory 帧的 value 特征向量。因此，key 向量被学习为匹配鲁棒的外观变化的视觉语义特征。另一方面，value 向量存储了非常详细的关于产生 MASK 预测的详细信息（如目标对象和目标边界）。query 帧和 memory 帧的 value 向量包含了用于不同目的的信息。具体地，query 帧的 value 向量被学习为存储详细的外观信息方便我们进行准确的解码目标 MASK。而 memory 帧的 value 向量被学习为编码视觉语义和关于每个特征是属于前景还是背景的 MASK 掩码信息。</p>
<p>query 帧和 memory 帧的 key 和 value 对输入到 Space-Time Memory Read 块。query 帧和 memory 帧的 key 向量的每一个“像素” 在视频的真个“时-空”空间上进行稠密的匹配。得到的相对匹配值被用于从 memory 帧中寻址 value 向量，组合这些向量得到一个输出向量。最终，decoder 把 Space-Time Memory Read 块的输出作为输入，重建 query 帧的 MASK.</p>
<h2 id="Key-和-Value-嵌入"><a href="#Key-和-Value-嵌入" class="headerlink" title="Key 和 Value 嵌入"></a>Key 和 Value 嵌入</h2><h2 id="Query-encoder"><a href="#Query-encoder" class="headerlink" title="Query encoder"></a>Query encoder</h2><p>query encoder 的输入是 query 帧，即当前帧图像，输出是两个特征图 key 和 value，它们是通过在 backbone 网络后增加两个并行的卷积层（没有非线性层）得到的，这些卷积层当做一个 bottleneck 层，约简 backbone 网络输出的特征图的通道大小，对于 key 特征通道数约简为 1&#x2F;8，对于 value 特征通道数约简为 1&#x2F;2。query 帧通过 query encoder 输出得到成对的 key 和 value 对：$k^Q \in \mathbb{R}^{H \times W \times C &#x2F; 8}, v^Q \in \mathbb{R}^{H \times W \times C &#x2F; 2}$，这里 $H$ 是高，$W$ 是宽，$C$ 是 backbone 网络输出的特征图的通道数。</p>
<h2 id="Memory-encoder"><a href="#Memory-encoder" class="headerlink" title="Memory encoder"></a>Memory encoder</h2><p>与 query encoder 相比，memory encoder 具有相同的结构，除了输入。memory encoder 的输入是图像帧（RGB 格式，三通道，取值 0~255）和对应的目标 MASK(GRAY 二值图，单通道，取值 0 或者 1) 沿着通道方向进行拼接得到的对象（4通道张量）。</p>
<p>对于包含有多个记忆帧的图像序列，每一个图像帧和 MASK 对分别独立输入 memory encoder 得到 key 和 value 特征对，然后沿着时间维度分别堆叠 key 和 value 向量，得到：$k^M \in \mathbb{R}^{T \times H \times W \times C &#x2F; 8}, v^M \in \mathbb{R}^{T \times H \times W \times C &#x2F; 2}$，这里 $T$ 是记忆帧中图像的个数。</p>
<p>文章采用 ResNet50 作为 backbone 网络用于 query encoder 和 memory encoder. 使用 stage-4（res4） 特征图作为基础特征图计算 query 帧和 memory 帧的 key 和 value 特征对。特别地，对于 memory encoder，通过对于第一个卷积层插入额外的单通道卷积核来方便处理 4-通道的张量（拼接的图像帧和对应的 MASK）。网络权重从 ImageNet 预训练模型初始化，除了新增加的单通道卷积核采用随机初始化。</p>
<h2 id="Space-Time-Memory-Read"><a href="#Space-Time-Memory-Read" class="headerlink" title="Space-Time Memory Read"></a>Space-Time Memory Read</h2><p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm-read.png?raw=true"></p>
<p>在记忆读取（Space-Time Memory Read） 中，首先计算 query 帧和 memory 帧的 key 向量在所有“像素”上的相似度，这种相似度匹配是通过比较 memory 帧 key 特征中的每个时空位置与 query 帧 key 特征中的每个空间位置，以非局部方式执行相似性匹配。然后，通过相似度权重的加权求和来检索 memory 帧的 value，并将其与 query 帧的 value 连接起来。这个记忆读取操作在查询（query）特征图上的每个位置，可以总结为：</p>

$$
\mathtt{y}_i = \lbrack \mathtt{v}_i^Q, {\frac{1}{Z}} \sum_{\forall j}f(\mathtt{k}_i^Q, \mathtt{k}_j^M)\mathtt{v}_j^M \rbrack \tag{1}
$$

<p>这里 $i,j$ 分别表示 query 和 memory 的位置索引，$Z&#x3D;\sum_{\forall j} f(\mathtt{k}_i^Q, \mathtt{k}_j^M)$ 是标准化因子，$\lbrack \cdot, \cdot \rbrack$ 表示拼接。相似度函数定义如下：</p>

$$
f(\mathtt{k}_i^Q, \mathtt{k}_j^M) = \text{exp}(\mathtt{k}_i^Q \circ \mathtt{k}_j^M) \tag{2}
$$

<p>这里 $\circ$ 记作点乘(dot-product)。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>解码器 decoder 接受记忆读取（Space-Time Memory Read）操作的输出并重建当前帧的对象掩码。文章使用 [<font color=blue> ref1</font>](Fast video object segmentation by referenceguided mask propagation)中使用的细化（refinement）模块作为解码器的构建块。首先通过卷积层和残差块将记忆读取（Space-Time Memory Read）的输出压缩为 256 个通道，然后通过多个细化模块逐步将压缩后的特征映射一次提升两倍。每个阶段的细化模块都通过残差连接以相应的比例从查询编码器（query encoder）获取前一阶段的输出和特征映射。最后一个细化块的输出用于通过最后的卷积层重建对象掩码，然后进行 softmax 操作。解码器中的每个卷积层都使用 3×3 滤波器，产生 256 通道输出，但最后一个产生 2 通道输出。解码器以输入图像的 1&#x2F;4 比例预测掩码。</p>
<h2 id="Multi-object-Segmentation"><a href="#Multi-object-Segmentation" class="headerlink" title="Multi-object Segmentation"></a>Multi-object Segmentation</h2><p>到此，文章针对单目标视频分割已经可以处理，但是对于多目标还不适用。不过，通过 MASK merging 操作能够扩展提出的框架以处理多目标视频分割。为每个对象独立运行上面的模型，并为所有对象计算掩码概率图。然后，使用类似于[<font color=blue> ref1</font>]的软聚合操作合并预测的映射。在[<font color=blue> ref1</font>]中，掩码合并仅在测试期间作为后处理步骤进行。在这项工作中，将操作作为差分网络层，并将其应用于训练和测试中。更多的细节参考文章补充材料。</p>
<h2 id="Two-stage-Training"><a href="#Two-stage-Training" class="headerlink" title="Two-stage Training"></a>Two-stage Training</h2><p>网络首先在静态图像数据生成的模拟数据集上进行预训练。然后，通过主训练对现实世界的视频进行进一步的微调。</p>
<h3 id="Pre-training-on-images"><a href="#Pre-training-on-images" class="headerlink" title="Pre-training on images"></a>Pre-training on images</h3><p>本文的框架的一个优点是，它不需要很长的训练视频。这是因为该方法在没有任何时间平滑假设的情况下学习了距离像素之间的语义时空匹配。这意味着可以只用几个帧来训练该网络。能够使用图像数据集模拟训练视频。[<font color=blue> ref1</font>] 使用静态图像训练他们的网络，本篇采取了类似的策略。通过对具有不同参数的静态图像应用随机仿射变换(rotation, sheering, zooming, translation, and cropping)，生成由 3 帧组成的合成视频剪辑。利用带有目标掩码注释的图像数据集(显著目标检测，语义分割)来预训练网络。通过这样做，期望模型对各种各样的对象外观和类别都是健壮的。</p>
<h3 id="Main-training-on-videos"><a href="#Main-training-on-videos" class="headerlink" title="Main training on videos"></a>Main training on videos</h3><p>预训练结束后，使用真实视频数据进行主训练。在这一步中，根据目标评估基准，使用 Youtube-VOS 或DAVIS-2017。为了制作一个训练样本，从一个训练视频中抽取 3 个临时有序的帧。为了了解长时间的外观变化，在采样期间随机跳过帧。在训练过程中，跳过的最大帧数与 curriculum learning 一样，从 0 帧逐渐增加到 25 帧。</p>
<h3 id="Training-with-dynamic-memory"><a href="#Training-with-dynamic-memory" class="headerlink" title="Training with dynamic memory"></a>Training with dynamic memory</h3><p>在训练过程中，记忆(memory)会根据网络之前的输出动态更新。当系统逐帧向前移动时，前一步计算的分段输出被添加到 memory 中以用于下一帧。未经阈值处理的原始网络输出成为前景目标的概率图，直接用于 memory 嵌入来建模估计的不确定性。</p>
<h3 id="Training-details"><a href="#Training-details" class="headerlink" title="Training details"></a>Training details</h3><p>文章使用随机裁剪的 384×384 补丁(patches)进行训练。对于所有实验，将 minibatch 大小设置为 4，并禁用所有 batch 规范化层。使用 Adam 优化器最小化交叉熵损失，固定学习率为 1e-5。预训练大约需要 4 天，主训练大约需要 3 天，使用 4 块 NVIDIA GeForce 1080 Ti gpu。</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>将所有之前的帧写入 memory 可能会引起实际问题，例如 GPU 记忆溢出和运行速度慢。相反，文章通过一个简单的规则来选择要放到 memory 中的帧。第一帧和前一帧是最重要的参考信息。第一帧总是提供可靠的信息，因为它是 ground-truth MASK。前一帧在外观上与当前帧相似，因此可以期望精确的像素匹配和掩码传播。因此，默认将这两帧放入 memory 中。</p>
<p>对于中间帧，只需每 N 帧保存一个新的 memory 帧。N 是一个控制速度和精度之间权衡的超参数，除非另有说明，否则使用 N &#x3D; 5。值得注意的是，框架在没有额外训练的情况下实现了在线学习和在线适应的效果。通过不更新模型参数，将之前的帧存储到 memory 中，很容易实现在线模型更新的效果。因此，文章提出的方法比以前的大多数方法运行得快得多，同时达到了最先进的精度。</p>
<h1 id="评估-Evaluation"><a href="#评估-Evaluation" class="headerlink" title="评估 Evaluation"></a>评估 Evaluation</h1><p>作者在 Youtube-VOS 和 DAVIS benchmark 上评估了模型。具体地，分别在这两个数据集的训练集上训练得到两个模型。对于 Youtube-VOS，使用官方的数据划分，训练集包含视频 3471 个。对于 DAVIS，使用 DAVIS-2017 的训练集的 60 个视频进行训练，分别在 DAVIS-2016 和 DAVIS-2017 上进行评估。除此之外，又额外使用 Youtube-VOS 进行训练后再在 DAVIS 上进行了评估，给出结果。所有结果都是直接从网络输出后没有进行后处理。</p>
<p>评估标准是 region similarity $\mathcal{J}$ 和 contour accuracy $\mathcal{F}$。</p>
<h2 id="Youtube-VOS"><a href="#Youtube-VOS" class="headerlink" title="Youtube-VOS"></a>Youtube-VOS</h2><p>Youtube-VOS 是用于视频对象分割的大规模数据集，由 4453 个带有多个对象注释的视频组成。该数据集比由 120 个视频组成的流行的 DAVIS 基准大 30 倍。它还具有未见对象类别（unseen object categories）的验证数据。因此，它可以很好地衡量不同算法的泛化性能（generalization performance）。验证集由 474 个视频组成，包括 91 个对象类别。它对 65 种可见和 26 种不可见的物体类别有单独的测量（separate measures）。</p>
<p>在 Youtube-VOS 验证集上的多目标视频分割结果，其中 <strong>STM</strong> 表示文章提出的方法：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td></td>
<td align="center">Seen</td>
<td align="center">Seen</td>
<td align="center">Unseen</td>
<td align="center">Unseen</td>
</tr>
<tr>
<td align="left"></td>
<td>Overall</td>
<td align="center">$\mathcal{J}$</td>
<td align="center">$\mathcal{F}$</td>
<td align="center">$\mathcal{J}$</td>
<td align="center">$\mathcal{F}$</td>
</tr>
<tr>
<td align="left">OSMN</td>
<td>51.2</td>
<td align="center">60.0</td>
<td align="center">60.1</td>
<td align="center">40.6</td>
<td align="center">44.0</td>
</tr>
<tr>
<td align="left">MSK</td>
<td>53.1</td>
<td align="center">59.9</td>
<td align="center">59.5</td>
<td align="center">45.0</td>
<td align="center">47.9</td>
</tr>
<tr>
<td align="left">RGMP</td>
<td>53.8</td>
<td align="center">59.5</td>
<td align="center">-</td>
<td align="center">45.2</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">OnAVOS</td>
<td>55.2</td>
<td align="center">60.1</td>
<td align="center">62.7</td>
<td align="center">46.6</td>
<td align="center">51.4</td>
</tr>
<tr>
<td align="left">RVOS</td>
<td>56.8</td>
<td align="center">63.6</td>
<td align="center">67.2</td>
<td align="center">45.5</td>
<td align="center">51.0</td>
</tr>
<tr>
<td align="left">OSVOS</td>
<td>58.8</td>
<td align="center">59.8</td>
<td align="center">60.5</td>
<td align="center">54.2</td>
<td align="center">60.7</td>
</tr>
<tr>
<td align="left">S2S</td>
<td>64.4</td>
<td align="center">71.0</td>
<td align="center">70.0</td>
<td align="center">55.5</td>
<td align="center">61.2</td>
</tr>
<tr>
<td align="left">A-GAME</td>
<td>66.1</td>
<td align="center">67.8</td>
<td align="center">-</td>
<td align="center">60.8</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">PreMVOS</td>
<td>66.9</td>
<td align="center">71.4</td>
<td align="center">75.9</td>
<td align="center">56.5</td>
<td align="center">63.7</td>
</tr>
<tr>
<td align="left">BoLTVOS</td>
<td>71.1</td>
<td align="center">71.6</td>
<td align="center">-</td>
<td align="center">64.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left"><strong>STM</strong></td>
<td><strong>79.4</strong></td>
<td align="center"><strong>79.7</strong></td>
<td align="center"><strong>84.2</strong></td>
<td align="center"><strong>72.8</strong></td>
<td align="center"><strong>80.9</strong></td>
</tr>
</tbody></table>
<h2 id="DAVIS"><a href="#DAVIS" class="headerlink" title="DAVIS"></a>DAVIS</h2><h3 id="单目标（DAVIS-2016）"><a href="#单目标（DAVIS-2016）" class="headerlink" title="单目标（DAVIS-2016）"></a>单目标（DAVIS-2016）</h3><p>使用包含 20 个视频的 DAVIS-2016 验证集，每个视频都为单个目标对象标注了高质量的掩码。在下表中，包含了在线学习的使用，并提供了每种方法的大致运行时间。以前大多数表现最好的方法都依赖于在线学习，这严重损害了运行速度。在所有没有在线学习的竞争方法中，文章提出的方法达到了最好的准确率，并且在很短的时间内显示出与基于在线学习的最佳方法的竞争结果。另外，使用来自 Youtube-VOS 的额外数据进行训练，在很大程度上优于所有方法。</p>
<p>下表是在 DAVIS-2016 验证集上的评估结果。OL 表示在线学习。(<strong>+YV</strong>) 表示额外在 Youtube-VOS 上训练。由于篇幅限制，$\mathcal{J}$ Mean 小于 79 的方法在本文中略去，完整的表格见文章补充资料：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td>OL</td>
<td align="center">$\mathcal{J}$ Mean</td>
<td align="center">$\mathcal{F}$ Mean</td>
<td align="center">Time</td>
</tr>
<tr>
<td align="left">S2S(<strong>+YV</strong>)</td>
<td>$\surd$</td>
<td align="center">79.1</td>
<td align="center">-</td>
<td align="center">9s</td>
</tr>
<tr>
<td align="left">MSK</td>
<td>$\surd$</td>
<td align="center">79.7</td>
<td align="center">75.4</td>
<td align="center">12s</td>
</tr>
<tr>
<td align="left">OSVOS</td>
<td>$\surd$</td>
<td align="center">79.8</td>
<td align="center">80.6</td>
<td align="center">9s</td>
</tr>
<tr>
<td align="left">MaskRNN</td>
<td>$\surd$</td>
<td align="center">80.7</td>
<td align="center">80.9</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">VideoMatch</td>
<td></td>
<td align="center">81.0</td>
<td align="center">-</td>
<td align="center">0.32s</td>
</tr>
<tr>
<td align="left">FEELVOS(<strong>+YV</strong>)</td>
<td></td>
<td align="center">81.1</td>
<td align="center">82.2</td>
<td align="center">0.45s</td>
</tr>
<tr>
<td align="left">RGMP</td>
<td></td>
<td align="center">81.5</td>
<td align="center">82.0</td>
<td align="center">0.13s</td>
</tr>
<tr>
<td align="left">A-GAME(<strong>+YV</strong>)</td>
<td></td>
<td align="center">82.0</td>
<td align="center">82.2</td>
<td align="center">0.07s</td>
</tr>
<tr>
<td align="left">FAVOS</td>
<td></td>
<td align="center">82.4</td>
<td align="center">79.5</td>
<td align="center">1.8s</td>
</tr>
<tr>
<td align="left">LSE</td>
<td>$\surd$</td>
<td align="center">82.9</td>
<td align="center">80.3</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">CINN</td>
<td>$\surd$</td>
<td align="center">83.4</td>
<td align="center">85.0</td>
<td align="center">&gt;30s</td>
</tr>
<tr>
<td align="left">PReMVOS</td>
<td>$\surd$</td>
<td align="center">84.9</td>
<td align="center">88.6</td>
<td align="center">&gt;30s</td>
</tr>
<tr>
<td align="left">$\text{OSVOS}^S$</td>
<td>$\surd$</td>
<td align="center">85.6</td>
<td align="center">86.4</td>
<td align="center">4.5s</td>
</tr>
<tr>
<td align="left">OnAVOS</td>
<td>$\surd$</td>
<td align="center">86.1</td>
<td align="center">84.9</td>
<td align="center">13s</td>
</tr>
<tr>
<td align="left">DyeNet</td>
<td>$\surd$</td>
<td align="center">86.2</td>
<td align="center">-</td>
<td align="center">2.32s</td>
</tr>
<tr>
<td align="left"><strong>STM</strong></td>
<td></td>
<td align="center">84.8</td>
<td align="center">88.1</td>
<td align="center">0.16s</td>
</tr>
<tr>
<td align="left"><strong>STM(+YV)</strong></td>
<td></td>
<td align="center"><strong>88.7</strong></td>
<td align="center"><strong>89.9</strong></td>
<td align="center"><strong>0.16s</strong></td>
</tr>
</tbody></table>
<h3 id="多目标（DAVIS-2017）"><a href="#多目标（DAVIS-2017）" class="headerlink" title="多目标（DAVIS-2017）"></a>多目标（DAVIS-2017）</h3><p>DAVIS-2017 是 DAVIS-2016 的多目标扩展。验证集包含 30 个视频 59 个目标。在下表中报告了验证集上的多目标视频分割结果。同样，文章提出的方法在没有在线学习的快速方法中表现出最好的性能。使用额外的 Youtube-VOS 数据，文章提出的方法在很大程度上优于之前所有最先进的方法，包括 2018 年 DAVIS 挑战赛的获胜者。在测试开发集上的结果包含在补充材料中。</p>
<p>DAVIS-2017 验证集的定量评价。OL 表示在线学习。(<strong>+YV</strong>) 表示使用 Youtube-VOS 进行额外。* 表示 $\mathcal{J}$ 均值与 $\mathcal{F}$ 均值的平均值:</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="left"></td>
<td>OL</td>
<td align="center">$\mathcal{J}$ Mean</td>
<td align="center">$\mathcal{F}$ Mean</td>
</tr>
<tr>
<td align="left">OSMN</td>
<td></td>
<td align="center">52.5</td>
<td align="center">57.1</td>
</tr>
<tr>
<td align="left">FAVOS</td>
<td></td>
<td align="center">54.6</td>
<td align="center">61.8</td>
</tr>
<tr>
<td align="left">VidMatch</td>
<td></td>
<td align="center">56.5</td>
<td align="center">68.2</td>
</tr>
<tr>
<td align="left">OSVOS</td>
<td>$\surd$</td>
<td align="center">56.6</td>
<td align="center">63.9</td>
</tr>
<tr>
<td align="left">MaskRNN</td>
<td>$\surd$</td>
<td align="center">60.5</td>
<td align="center">-</td>
</tr>
<tr>
<td align="left">OnAVOS</td>
<td>$\surd$</td>
<td align="center">64.5</td>
<td align="center">71.2</td>
</tr>
<tr>
<td align="left">$\text{OSVOS}^S$</td>
<td>$\surd$</td>
<td align="center">64.7</td>
<td align="center">71.3</td>
</tr>
<tr>
<td align="left">RGMP</td>
<td></td>
<td align="center">64.8</td>
<td align="center">68.6</td>
</tr>
<tr>
<td align="left">CINN</td>
<td>$\surd$</td>
<td align="center">67.2</td>
<td align="center">74.2</td>
</tr>
<tr>
<td align="left">A-GAME(<strong>+YV</strong>)</td>
<td></td>
<td align="center">67.2</td>
<td align="center">72.7</td>
</tr>
<tr>
<td align="left">FEELVOS(<strong>+YV</strong>)</td>
<td></td>
<td align="center">69.1</td>
<td align="center">74.0</td>
</tr>
<tr>
<td align="left">DyeNet</td>
<td>$\surd$</td>
<td align="center">*74.1</td>
<td align="center"></td>
</tr>
<tr>
<td align="left">PReMVOS</td>
<td>$\surd$</td>
<td align="center">73.9</td>
<td align="center">81.7</td>
</tr>
<tr>
<td align="left"><strong>STM</strong></td>
<td></td>
<td align="center">69.2</td>
<td align="center">74.0</td>
</tr>
<tr>
<td align="left"><strong>STM(+YV)</strong></td>
<td></td>
<td align="center"><strong>79.2</strong></td>
<td align="center"><strong>84.3</strong></td>
</tr>
</tbody></table>
<p>使用额外训练数据带来的巨大性能飞跃表明，由于过度拟合，DAVIS 太小而无法训练可推广的深度网络。它还解释了为什么 DAVIS 基准测试中表现最佳的在线学习方法在大规模 Youtube-VOS 基准测试中没有表现出良好的性能。在线学习方法很难得到大量训练数据的帮助。这些方法通常需要大量的参数搜索(如数据合成方法、优化迭代、学习率和后处理），这对于大规模基准测试来说并不容易做到。)</p>
<h2 id="定性结果"><a href="#定性结果" class="headerlink" title="定性结果"></a>定性结果</h2><p>下图显示了文章结果的定性示例。从 Youtube-VOS 和 DAVIS 验证集中选择具有挑战性的视频，并对重要帧进行采样（例如遮挡之前和之后）。如图所示，文章提出的方法对于遮挡和复杂运动具有鲁棒性。更多结果请访问补充材料。</p>
<p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm-qualitative-results.png?raw=true"></p>
<h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><p>通过两个训练阶段训练模型：静态图像的预训练和使用 DAVIS 或 Youtube-VOS 的主训练。在下表中，比较了文章的方法与不同训练数据的性能。此外，提供跨数据集验证来衡量泛化性能。</p>
<p>Youtube-VOS 和 DAVIS-2017 验证集上的训练数据分析。比较通过不同训练阶段训练的模型。此外，报告了交叉验证结果（即使用在 Youtube-VOS 上训练的模型评估 DAVIS，反之亦然）。</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Variants</td>
<td align="center">Youtube-VOS</td>
<td align="center">DAVIS-2017</td>
<td align="center">DAVIS-2017</td>
</tr>
<tr>
<td align="left"></td>
<td align="center">Overall</td>
<td align="center">$\mathcal{J}$</td>
<td align="center">$\mathcal{F}$</td>
</tr>
<tr>
<td align="left">Pre-training only</td>
<td align="center">69.1</td>
<td align="center">57.9</td>
<td align="center">62.1</td>
</tr>
<tr>
<td align="left">Main-training only</td>
<td align="center">68.2</td>
<td align="center">38.1</td>
<td align="center">47.9</td>
</tr>
<tr>
<td align="left">Full training</td>
<td align="center"><strong>79.4</strong></td>
<td align="center">69.2</td>
<td align="center">74.0</td>
</tr>
<tr>
<td align="left">Cross validation</td>
<td align="center">56.3</td>
<td align="center"><strong>78.6</strong></td>
<td align="center"><strong>83.5</strong></td>
</tr>
</tbody></table>
<p><strong>仅预训练（Pre-training only）</strong>。有趣的是，在不使用任何真实视频的情况下，仅预训练模型优于仅主训练模型以及 YouTube-VOS 上的所有其他方法。然而，通过使用这两种训练策略获得了最大的性能。</p>
<p><strong>仅主要训练（Main-training only）</strong>。如果没有预训练阶段，模型在 Youtube-VOS 上的总体得分下降了 11.2。这表明尽管 Youtube-VOS 提供了超过 3000 个训练视频，但训练视频数据量仍然不足以发挥文章提出的网络的潜力。此外，DAVIS 上的性能非常低意味着存在严重的过度拟合问题，因为训练损失与完整模型相似（没有应用提前停止）。推测在预训练过程中遇到的不同对象有助于模型的泛化并防止过度拟合。</p>
<p><strong>交叉验证（Cross validation）</strong>。评估在 DAVIS 和 Youtube-VOS 上训练的模型，反之亦然。在 DAVIS 上训练的模型在 Youtube-VOS 上表现有限。这是预期的结果，因为 DAVIS 太小，无法学习对其他数据集的泛化能力。另一方面，我们在 Youtube-VOS 上训练的模型在 DAVIS 上表现良好，并且优于所有其他方法。</p>
<h2 id="记忆管理（Memory-Management）"><a href="#记忆管理（Memory-Management）" class="headerlink" title="记忆管理（Memory Management）"></a>记忆管理（Memory Management）</h2><p>为了最小的记忆消耗和最快的运行时间，可以将第一帧和&#x2F;或前一帧保存在记忆（memory）中。为了获得最大的准确度，最终模型除了第一帧和前一帧之外，每 5 帧保存一个新的中间记忆帧。</p>
<p>在下表中比较了不同的记忆管理规则。将第一个帧和前一个帧都保存到记忆中是最重要的，文章的模型使用两个记忆帧实现了最先进的精度。这是因为模型足够强大，可以处理较大的外观变化，同时通过有效利用记忆对漂移和错误积累具有鲁棒性。最重要的是，拥有中间帧存储器可以通过解决极具挑战性的情况进一步提高性能，如下图所示：</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Memory frame(s)</td>
<td align="center">Youtube-VOS</td>
<td align="center">DAVIS-2016</td>
<td align="center">DAVIS-2017</td>
<td align="center">Time</td>
</tr>
<tr>
<td align="left">First</td>
<td align="center">68.9</td>
<td align="center">81.4</td>
<td align="center">67.0</td>
<td align="center">0.06</td>
</tr>
<tr>
<td align="left">Previous</td>
<td align="center">69.7</td>
<td align="center">83.2</td>
<td align="center">69.6</td>
<td align="center">0.06</td>
</tr>
<tr>
<td align="left">First &amp; Previous</td>
<td align="center">78.4</td>
<td align="center">87.8</td>
<td align="center">77.7</td>
<td align="center">0.07</td>
</tr>
<tr>
<td align="left">Every 5 frames</td>
<td align="center"><strong>79.4</strong></td>
<td align="center"><strong>88.7</strong></td>
<td align="center"><strong>79.2</strong></td>
<td align="center">0.16</td>
</tr>
</tbody></table>
<p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm-intermediate-memory.png?raw=true"></p>
<p>为了进行更深入的分析，在下图中显示了帧级精度分布。对所有视频帧中所有对象的 Jaccard 分数进行排序，并绘制分数以分析在具有挑战性的场景中的性能。将最终模型（每 5 帧）与 First 和 Previous 进行比较，以检查使用额外记忆帧的效果。虽然两种设置在成功范围（超过 30%）上表现同样出色，但对于困难情况（低于 30%），额外记忆帧的效果变得清晰可见。 10% 和 30% 之间的巨大准确度差距表明，文章提出的网络可以通过额外的记忆帧更好地处理具有挑战性的情况。比较“仅第一个”和“仅前一个”，前一帧看起来对于处理失败情况更有用。</p>
<p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm-jaccard-score-on-davis-2017.png?raw=true"></p>
<p><strong>记忆可视化</strong>。在下图中，可视化了记忆读取操作，以验证学习到的时空匹配。从可视化中可以看出，记忆读取操作准确匹配查询和记忆帧之间的相应像素。</p>
<p><img src="https://github.com/xujinzh/archive/blob/master/images/vos/stm/stm-vis-space-time-read.png?raw=true"></p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>文章提出了一种用于半监督视频对象分割的新颖时空记忆网络，在准确性和速度方面都具有较好的表现。所提出的时空记忆网络有很大潜力成为许多其他像素级估计问题的突破。框架的预期能够应用在其他领域研究中，包括对象跟踪、交互式图像&#x2F;视频分割和修复（inpainting）。</p>
<h1 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h1><ol>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9139391">Space-Time Memory Networks for Video Object Segmentation With User Guidance</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://xujinzh.github.io">Jinzhong Xu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://xujinzh.github.io/2024/02/28/vos-stm-network-iccv2019/">https://xujinzh.github.io/2024/02/28/vos-stm-network-iccv2019/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xujinzh.github.io" target="_blank">J. Xu</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/video/">video</a><a class="post-meta__tags" href="/tags/vos/">vos</a><a class="post-meta__tags" href="/tags/STM/">STM</a></div><div class="post_share"><div class="social-share" data-image="/img/c30.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/03/04/vos-davis-eval/" title="Video Object Segmentation 评估指标"><img class="cover" src="/img/c6.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Video Object Segmentation 评估指标</div></div></a></div><div class="next-post pull-right"><a href="/2024/02/04/python-download-speedup/" title="利用 Python 加速下载大文件突破 IP 限速"><img class="cover" src="/img/c10.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">利用 Python 加速下载大文件突破 IP 限速</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/03/04/vos-davis-eval/" title="Video Object Segmentation 评估指标"><img class="cover" src="/img/c6.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-04</div><div class="title">Video Object Segmentation 评估指标</div></div></a></div><div><a href="/2022/02/19/ffmpeg-video-processing/" title="FFmpeg 视频处理等相关"><img class="cover" src="/img/c21.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-19</div><div class="title">FFmpeg 视频处理等相关</div></div></a></div><div><a href="/2022/03/24/python-opencv-video-info/" title="使用 python 的 opencv 获取视频信息"><img class="cover" src="/img/c4.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-24</div><div class="title">使用 python 的 opencv 获取视频信息</div></div></a></div><div><a href="/2020/07/11/youtube-dl/" title="youtube-dl 下载视频方法"><img class="cover" src="/img/c25.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-11</div><div class="title">youtube-dl 下载视频方法</div></div></a></div><div><a href="/2022/03/08/you-get-download-videos/" title="you-get 下载视频的方法"><img class="cover" src="/img/c24.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-08</div><div class="title">you-get 下载视频的方法</div></div></a></div><div><a href="/2024/03/19/vos-cfbi-cvpr2020-tpami2021/" title="CFBI+, Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration"><img class="cover" src="/img/c7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-19</div><div class="title">CFBI+, Collaborative Video Object Segmentation by Multi-Scale Foreground-Background Integration</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/silence.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jinzhong Xu</div><div class="author-info__description">众妙之门</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">405</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">316</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xujinzh"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xujinzh" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:xujinzhong027@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.mathscv.com" target="_blank" title="MathsCVBlog"><i class="fab fa-j"></i></a><a class="social-icon" href="https://xujinzh.github.io" target="_blank" title="GitHubBlog"><i class="fab fa-github-alt"></i></a><a class="social-icon" href="https://www.mathscv.com/power" target="_blank" title="MathsCVPower"><i class="fab fa-m"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">日本核污染水强排入海！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E8%B5%84%E6%BA%90"><span class="toc-number">1.</span> <span class="toc-text">文章资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%98%E7%9B%AE"><span class="toc-number">2.</span> <span class="toc-text">题目</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%9C%E8%80%85"><span class="toc-number">3.</span> <span class="toc-text">作者</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">4.</span> <span class="toc-text">问题定义</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E5%85%B3%E9%94%AE%E7%82%B9%E7%A4%BA%E6%84%8F%E5%9B%BE"><span class="toc-number">5.</span> <span class="toc-text">算法关键点示意图</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%83%B3%E6%B3%95%E5%BC%95%E5%87%BA"><span class="toc-number">6.</span> <span class="toc-text">想法引出</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">7.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8A%E7%9B%91%E7%9D%A3%E8%A7%86%E9%A2%91%E7%9B%AE%E6%A0%87%E5%88%86%E5%89%B2"><span class="toc-number">7.1.</span> <span class="toc-text">半监督视频目标分割</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C"><span class="toc-number">7.2.</span> <span class="toc-text">记忆网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Space-Time-Memory-Networks-STM"><span class="toc-number">8.</span> <span class="toc-text">Space-Time Memory Networks(STM)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Key-%E5%92%8C-Value-%E5%B5%8C%E5%85%A5"><span class="toc-number">8.1.</span> <span class="toc-text">Key 和 Value 嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Query-encoder"><span class="toc-number">8.2.</span> <span class="toc-text">Query encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Memory-encoder"><span class="toc-number">8.3.</span> <span class="toc-text">Memory encoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Space-Time-Memory-Read"><span class="toc-number">8.4.</span> <span class="toc-text">Space-Time Memory Read</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder"><span class="toc-number">8.5.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-object-Segmentation"><span class="toc-number">8.6.</span> <span class="toc-text">Multi-object Segmentation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-stage-Training"><span class="toc-number">8.7.</span> <span class="toc-text">Two-stage Training</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-training-on-images"><span class="toc-number">8.7.1.</span> <span class="toc-text">Pre-training on images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Main-training-on-videos"><span class="toc-number">8.7.2.</span> <span class="toc-text">Main training on videos</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-with-dynamic-memory"><span class="toc-number">8.7.3.</span> <span class="toc-text">Training with dynamic memory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-details"><span class="toc-number">8.7.4.</span> <span class="toc-text">Training details</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Inference"><span class="toc-number">8.8.</span> <span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0-Evaluation"><span class="toc-number">9.</span> <span class="toc-text">评估 Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Youtube-VOS"><span class="toc-number">9.1.</span> <span class="toc-text">Youtube-VOS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DAVIS"><span class="toc-number">9.2.</span> <span class="toc-text">DAVIS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E7%9B%AE%E6%A0%87%EF%BC%88DAVIS-2016%EF%BC%89"><span class="toc-number">9.2.1.</span> <span class="toc-text">单目标（DAVIS-2016）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%9B%AE%E6%A0%87%EF%BC%88DAVIS-2017%EF%BC%89"><span class="toc-number">9.2.2.</span> <span class="toc-text">多目标（DAVIS-2017）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E6%80%A7%E7%BB%93%E6%9E%9C"><span class="toc-number">9.3.</span> <span class="toc-text">定性结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-number">10.</span> <span class="toc-text">分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">10.1.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86%EF%BC%88Memory-Management%EF%BC%89"><span class="toc-number">10.2.</span> <span class="toc-text">记忆管理（Memory Management）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">11.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%A9%E5%B1%95"><span class="toc-number">12.</span> <span class="toc-text">扩展</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/26/robot-jetson-librealsense-realsense-ros/" title="jetson 安装 librealsense 和 realsense-ros"><img src="/img/c3.jpeg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="jetson 安装 librealsense 和 realsense-ros"/></a><div class="content"><a class="title" href="/2024/07/26/robot-jetson-librealsense-realsense-ros/" title="jetson 安装 librealsense 和 realsense-ros">jetson 安装 librealsense 和 realsense-ros</a><time datetime="2024-07-26T10:26:59.000Z" title="发表于 2024-07-26 18:26:59">2024-07-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/18/robot-jetson-max-performance/" title="jetson orin NX 开启最大性能"><img src="/img/c9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="jetson orin NX 开启最大性能"/></a><div class="content"><a class="title" href="/2024/07/18/robot-jetson-max-performance/" title="jetson orin NX 开启最大性能">jetson orin NX 开启最大性能</a><time datetime="2024-07-18T03:30:55.000Z" title="发表于 2024-07-18 11:30:55">2024-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/16/linux-limit-tcp-speed-by-trickle/" title="使用 trickle 限制 linux 上应用程序的下载和上传速度"><img src="/img/c9.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="使用 trickle 限制 linux 上应用程序的下载和上传速度"/></a><div class="content"><a class="title" href="/2024/07/16/linux-limit-tcp-speed-by-trickle/" title="使用 trickle 限制 linux 上应用程序的下载和上传速度">使用 trickle 限制 linux 上应用程序的下载和上传速度</a><time datetime="2024-07-16T02:01:08.000Z" title="发表于 2024-07-16 10:01:08">2024-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/09/ai-pytorch-model-infer-acc-onnx-tensorrt/" title="使用 tensorrt 加速 pytorch 模型推理"><img src="/img/c16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="使用 tensorrt 加速 pytorch 模型推理"/></a><div class="content"><a class="title" href="/2024/07/09/ai-pytorch-model-infer-acc-onnx-tensorrt/" title="使用 tensorrt 加速 pytorch 模型推理">使用 tensorrt 加速 pytorch 模型推理</a><time datetime="2024-07-09T11:25:01.000Z" title="发表于 2024-07-09 19:25:01">2024-07-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/09/linux-vlc-root/" title="Ubuntu 上安装 vlc 并使用 root 用户播放"><img src="/img/c8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Ubuntu 上安装 vlc 并使用 root 用户播放"/></a><div class="content"><a class="title" href="/2024/07/09/linux-vlc-root/" title="Ubuntu 上安装 vlc 并使用 root 用户播放">Ubuntu 上安装 vlc 并使用 root 用户播放</a><time datetime="2024-07-09T01:05:21.000Z" title="发表于 2024-07-09 09:05:21">2024-07-09</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By Jinzhong Xu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '695ca5d39e2ba2f9feb5',
      clientSecret: '9d4027af6364ff54595b7a8580977ec58c38a5ae',
      repo: 'xujinzh.github.io',
      owner: 'xujinzh',
      admin: ['xujinzh'],
      id: '0374059c7e7c019a34b35ae5dbf3c715',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="众,妙,之,门" data-fontsize="15px" data-random="false" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>